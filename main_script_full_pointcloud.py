"""
PointNet å›å½’è®­ç»ƒè„šæœ¬ï¼ˆå®Œæ•´ç‚¹äº‘ç‰ˆæœ¬ï¼‰
ä½¿ç”¨å®Œæ•´ç‚¹äº‘ä½œä¸ºè¾“å…¥ï¼Œé¢„æµ‹9ä¸ªåœ°æ ‡ç‚¹åæ ‡
"""
import os
import sys

# æ·»åŠ å½“å‰ç›®å½•åˆ° Python è·¯å¾„
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
if BASE_DIR not in sys.path:
    sys.path.insert(0, BASE_DIR)

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import TensorDataset, DataLoader, random_split
from tqdm import tqdm
from pointnet_utils import PointNetEncoder, feature_transform_reguliarzer

# =========================================================
# é…ç½®
# =========================================================
# é€‰é¡¹1: ä»é¡¹ç›®ç›®å½•è¯»å–ï¼ˆæ¨èï¼‰
EXPORT_ROOT = os.path.join(BASE_DIR, 'data', 'pointcloud')

# é€‰é¡¹2: ä»ç½‘ç»œè·¯å¾„è¯»å–ï¼ˆå¦‚æœæ•°æ®åœ¨ç½‘ç»œè·¯å¾„ï¼Œå–æ¶ˆä¸‹é¢çš„æ³¨é‡Šï¼‰
# EXPORT_ROOT = r"\\uz\data\Admin\mka\results\hou-and-hu\vs\pointcloud"

LABELS_FILE = os.path.join(BASE_DIR, 'labels.csv')
PROJECTS_LIST_FILE = os.path.join(BASE_DIR, 'valid_projects.txt')

NUM_TARGET_POINTS = 9
OUTPUT_DIM = NUM_TARGET_POINTS * 3  # 27ç»´

# GPU é…ç½®
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}")
if torch.cuda.is_available():
    print(f"   GPUåç§°: {torch.cuda.get_device_name(0)}")
    print(f"   GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")

# =========================================================
# è®­ç»ƒå‚æ•°é…ç½®
# =========================================================
BATCH_SIZE = 8               # ç‚¹äº‘è¾ƒå¤§ï¼ˆå¹³å‡19345ç‚¹ï¼‰ï¼Œä½¿ç”¨è¾ƒå°æ‰¹æ¬¡ä»¥é¿å…å†…å­˜ä¸è¶³
NUM_EPOCHS = 500
LEARNING_RATE = 0.001
LR_DECAY_STEP = 150
LR_DECAY_GAMMA = 0.5
DROPOUT_RATE = 0.3
FEATURE_TRANSFORM_WEIGHT = 0.001

# è®­ç»ƒ/éªŒè¯é›†åˆ’åˆ†
TRAIN_RATIO = 0.8

# æ¨¡å‹ä¿å­˜é…ç½®
MODEL_NAME = 'pointnet_regression_model_full.pth'
BEST_MODEL_NAME = 'pointnet_regression_model_full_best.pth'

# =========================================================
# æ¨¡å‹å®šä¹‰
# =========================================================
class PointNetRegressor(nn.Module):
    def __init__(self, output_dim=27, dropout_rate=0.3):
        super(PointNetRegressor, self).__init__()
        # PointNetEncoder å¯ä»¥å¤„ç†ä»»æ„æ•°é‡çš„ç‚¹
        self.feat = PointNetEncoder(global_feat=True, feature_transform=True, channel=3)
        self.fc1 = nn.Linear(1024, 512)
        self.fc2 = nn.Linear(512, 256)
        self.fc3 = nn.Linear(256, output_dim)
        self.bn1 = nn.BatchNorm1d(512)
        self.bn2 = nn.BatchNorm1d(256)
        self.dropout = nn.Dropout(p=dropout_rate)
        self.relu = nn.ReLU()

    def forward(self, x):
        # x shape: (batch_size, 3, num_points)
        x, trans, trans_feat = self.feat(x)
        x = self.relu(self.bn1(self.fc1(x)))
        x = self.relu(self.bn2(self.dropout(self.fc2(x))))
        x = self.fc3(x)
        return x, trans_feat

# =========================================================
# æ•°æ®åŠ è½½ï¼ˆå®Œæ•´ç‚¹äº‘ç‰ˆæœ¬ï¼‰
# =========================================================
def load_data():
    """
    åŠ è½½å®Œæ•´ç‚¹äº‘å’Œæ ‡ç­¾
    è¿”å›: X (torch.Tensor), Y (torch.Tensor)
    """
    print("--- æ­£åœ¨åŠ è½½å®Œæ•´ç‚¹äº‘å’Œæ ‡ç­¾ ---")
    
    # è¯»å–é¡¹ç›®åˆ—è¡¨
    if not os.path.exists(PROJECTS_LIST_FILE):
        raise FileNotFoundError(f"âŒ é¡¹ç›®åˆ—è¡¨æ–‡ä»¶ä¸å­˜åœ¨: {PROJECTS_LIST_FILE}\nè¯·å…ˆè¿è¡Œ create_labels_from_npy.py")
    
    with open(PROJECTS_LIST_FILE, 'r', encoding='utf-8') as f:
        project_names = [line.strip() for line in f if line.strip()]
    
    # è¯»å–æ ‡ç­¾æ–‡ä»¶
    if not os.path.exists(LABELS_FILE):
        raise FileNotFoundError(f"âŒ æ ‡ç­¾æ–‡ä»¶ä¸å­˜åœ¨: {LABELS_FILE}\nè¯·å…ˆè¿è¡Œ create_labels_from_npy.py")
    
    labels_df = pd.read_csv(LABELS_FILE, header=None)
    all_labels_np = labels_df.values.astype(np.float32)
    
    if len(project_names) != len(all_labels_np):
        print(f"âš ï¸  è­¦å‘Š: é¡¹ç›®æ•°é‡ ({len(project_names)}) ä¸æ ‡ç­¾æ•°é‡ ({len(all_labels_np)}) ä¸åŒ¹é…")
        min_len = min(len(project_names), len(all_labels_np))
        project_names = project_names[:min_len]
        all_labels_np = all_labels_np[:min_len]
    
    valid_features = []
    valid_labels = []
    point_counts = []
    
    print(f"åŠ è½½ {len(project_names)} ä¸ªæ ·æœ¬çš„ç‚¹äº‘...")
    
    for i, project_name in enumerate(tqdm(project_names, desc="åŠ è½½ç‚¹äº‘")):
        project_dir = os.path.join(EXPORT_ROOT, project_name)
        pointcloud_file = os.path.join(project_dir, "pointcloud_full.npy")
        
        if not os.path.exists(pointcloud_file):
            print(f"âš ï¸  è·³è¿‡ {project_name}: æœªæ‰¾åˆ° pointcloud_full.npy")
            continue
        
        try:
            # åŠ è½½å®Œæ•´ç‚¹äº‘
            pointcloud = np.load(pointcloud_file).astype(np.float32)  # shape: (N, 3)
            
            if len(pointcloud) == 0:
                print(f"âš ï¸  è·³è¿‡ {project_name}: ç‚¹äº‘ä¸ºç©º")
                continue
            
            # å…ˆè·å–æ ‡ç­¾
            current_label = all_labels_np[i].reshape(NUM_TARGET_POINTS, 3)
            
            # ä½¿ç”¨åœ°æ ‡ç‚¹è´¨å¿ƒä½œä¸ºå‚è€ƒç‚¹ï¼ˆè€Œä¸æ˜¯ç‚¹äº‘è´¨å¿ƒï¼‰
            # è¿™æ ·å¯ä»¥ç¡®ä¿ç‚¹äº‘å’Œåœ°æ ‡ç‚¹ä½¿ç”¨ç›¸åŒçš„å‚è€ƒåæ ‡ç³»
            label_centroid = np.mean(current_label, axis=0)
            
            # ç‚¹äº‘å’Œæ ‡ç­¾éƒ½ç›¸å¯¹äºåœ°æ ‡ç‚¹è´¨å¿ƒ
            centered_pointcloud = pointcloud - label_centroid
            centered_label = current_label - label_centroid
            
            # å½’ä¸€åŒ–ï¼šä½¿ç”¨ç‚¹äº‘çš„æ ‡å‡†å·®è¿›è¡Œç¼©æ”¾
            # è¿™æ ·å¯ä»¥ç»Ÿä¸€ä¸åŒæ ·æœ¬çš„å°ºåº¦
            scale = np.std(centered_pointcloud)
            if scale > 1e-6:  # é¿å…é™¤é›¶
                centered_pointcloud = centered_pointcloud / scale
                centered_label = centered_label / scale
            
            # è½¬ç½®ä¸º (3, N) æ ¼å¼ï¼Œé€‚é… PointNet
            # PointNet æœŸæœ›è¾“å…¥æ ¼å¼: (batch_size, 3, num_points)
            centered_pointcloud_T = centered_pointcloud.T  # (3, N)
            
            valid_features.append(centered_pointcloud_T)
            valid_labels.append(centered_label.flatten())
            point_counts.append(len(pointcloud))
            
        except Exception as e:
            print(f"âŒ å¤„ç† {project_name} æ—¶å‡ºé”™: {e}")
            continue
    
    if not valid_features:
        raise RuntimeError("âŒ æœªèƒ½åŠ è½½ä»»ä½•æœ‰æ•ˆæ•°æ®ï¼")
    
    print(f"\nâœ… æˆåŠŸåŠ è½½ {len(valid_features)} ä¸ªæ ·æœ¬")
    print(f"   ç‚¹äº‘æ•°é‡ç»Ÿè®¡:")
    print(f"     æœ€å°: {min(point_counts)} ä¸ªç‚¹")
    print(f"     æœ€å¤§: {max(point_counts)} ä¸ªç‚¹")
    print(f"     å¹³å‡: {np.mean(point_counts):.0f} ä¸ªç‚¹")
    
    # æ³¨æ„ï¼šç”±äºæ¯ä¸ªæ ·æœ¬çš„ç‚¹æ•°ä¸åŒï¼Œæˆ‘ä»¬éœ€è¦ç»Ÿä¸€å¤„ç†
    # æ–¹æ³•1: ä½¿ç”¨æœ€å¤§ç‚¹æ•°ï¼Œä¸è¶³çš„ç”¨0å¡«å……ï¼ˆä¸æ¨èï¼Œæµªè´¹å†…å­˜ï¼‰
    # æ–¹æ³•2: ä½¿ç”¨å›ºå®šç‚¹æ•°ï¼Œéšæœºé‡‡æ ·æˆ–æˆªå–ï¼ˆæ¨èï¼‰
    # æ–¹æ³•3: ä½¿ç”¨åŠ¨æ€æ‰¹å¤„ç†ï¼ˆå¤æ‚ï¼‰
    
    # è¿™é‡Œä½¿ç”¨æ–¹æ³•2ï¼šç»Ÿä¸€é‡‡æ ·åˆ°å›ºå®šç‚¹æ•°
    # æ ¹æ®ç‚¹äº‘ç»Ÿè®¡ï¼šå¹³å‡19345ç‚¹ï¼Œæœ€å°11564ç‚¹ï¼Œæœ€å¤§29182ç‚¹
    # ä½¿ç”¨8192ç‚¹å¯ä»¥ä¿ç•™æ›´å¤šç»†èŠ‚ï¼ŒåŒæ—¶ä¿æŒè®¡ç®—æ•ˆç‡
    MAX_POINTS = 8192  # ä¼˜åŒ–ï¼šä»2048å¢åŠ åˆ°8192ä»¥ä¿ç•™æ›´å¤šç‚¹äº‘ç»†èŠ‚
    print(f"\nç»Ÿä¸€é‡‡æ ·åˆ° {MAX_POINTS} ä¸ªç‚¹...")
    
    processed_features = []
    for feat in valid_features:
        num_points = feat.shape[1]
        if num_points >= MAX_POINTS:
            # éšæœºé‡‡æ ·
            indices = np.random.choice(num_points, MAX_POINTS, replace=False)
            sampled_feat = feat[:, indices]
        else:
            # é‡å¤é‡‡æ ·ï¼ˆæœ‰æ”¾å›ï¼‰
            indices = np.random.choice(num_points, MAX_POINTS, replace=True)
            sampled_feat = feat[:, indices]
        processed_features.append(sampled_feat)
    
    # è½¬æ¢ä¸ºnumpyæ•°ç»„å¹¶è½¬ç½®ä¸º (N, 3, MAX_POINTS)
    X_np = np.array(processed_features, dtype=np.float32)  # (N, 3, MAX_POINTS)
    Y_np = np.array(valid_labels, dtype=np.float32)  # (N, 27)
    
    print(f"   æœ€ç»ˆæ•°æ®å½¢çŠ¶: X={X_np.shape}, Y={Y_np.shape}")
    
    return torch.from_numpy(X_np), torch.from_numpy(Y_np)

# =========================================================
# è®­ç»ƒå‡½æ•°
# =========================================================
def train():
    X, Y = load_data()
    if X is None:
        return
    
    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    dataset = TensorDataset(X, Y)
    train_size = int(TRAIN_RATIO * len(dataset))
    val_size = len(dataset) - train_size
    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])
    
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)
    
    print(f"\nğŸ“Š æ•°æ®é›†åˆ’åˆ†: è®­ç»ƒé›† {train_size} ä¸ªæ ·æœ¬, éªŒè¯é›† {val_size} ä¸ªæ ·æœ¬")
    
    model = PointNetRegressor(output_dim=OUTPUT_DIM, dropout_rate=DROPOUT_RATE).to(device)
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = torch.optim.lr_scheduler.StepLR(
        optimizer, step_size=LR_DECAY_STEP, gamma=LR_DECAY_GAMMA
    )
    
    print(f"\n--- å¼€å§‹è®­ç»ƒ {NUM_EPOCHS} è½® ---")
    print(f"ğŸ“‹ è®­ç»ƒé…ç½®:")
    print(f"   æ‰¹æ¬¡å¤§å°: {BATCH_SIZE}")
    print(f"   åˆå§‹å­¦ä¹ ç‡: {LEARNING_RATE}")
    print(f"   å­¦ä¹ ç‡è¡°å‡: æ¯ {LR_DECAY_STEP} è½® Ã— {LR_DECAY_GAMMA}")
    print(f"   Dropout: {DROPOUT_RATE}")
    print(f"   ç‰¹å¾å˜æ¢æ­£åˆ™åŒ–æƒé‡: {FEATURE_TRANSFORM_WEIGHT}")
    
    model.train()
    best_val_loss = float('inf')
    training_history = {'train_loss': [], 'val_loss': [], 'epoch': []}
    
    for epoch in range(NUM_EPOCHS):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        total_train_loss = 0
        train_count = 0
        
        for data, target in train_loader:
            data, target = data.to(device), target.to(device)
            optimizer.zero_grad()
            pred, trans_feat = model(data)
            loss = criterion(pred, target)
            if trans_feat is not None:
                loss += feature_transform_reguliarzer(trans_feat) * FEATURE_TRANSFORM_WEIGHT
            loss.backward()
            optimizer.step()
            total_train_loss += loss.item()
            train_count += 1
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        total_val_loss = 0
        val_count = 0
        
        with torch.no_grad():
            for data, target in val_loader:
                data, target = data.to(device), target.to(device)
                pred, trans_feat = model(data)
                loss = criterion(pred, target)
                if trans_feat is not None:
                    loss += feature_transform_reguliarzer(trans_feat) * FEATURE_TRANSFORM_WEIGHT
                total_val_loss += loss.item()
                val_count += 1
        
        avg_train_loss = total_train_loss / train_count
        avg_val_loss = total_val_loss / val_count if val_count > 0 else 0
        
        training_history['train_loss'].append(avg_train_loss)
        training_history['val_loss'].append(avg_val_loss)
        training_history['epoch'].append(epoch + 1)
        
        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step()
        current_lr = optimizer.param_groups[0]['lr']
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            best_model_path = os.path.join(BASE_DIR, BEST_MODEL_NAME)
            torch.save(model.state_dict(), best_model_path)
        
        # æ‰“å°è¿›åº¦
        if (epoch+1) % 25 == 0 or epoch == 0:
            print(f"Epoch {epoch+1:4d}/{NUM_EPOCHS} | "
                  f"Train Loss: {avg_train_loss:.6f} | "
                  f"Val Loss: {avg_val_loss:.6f} | "
                  f"LR: {current_lr:.6f} | "
                  f"Best Val: {best_val_loss:.6f}")
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    model_path = os.path.join(BASE_DIR, MODEL_NAME)
    torch.save(model.state_dict(), model_path)
    
    print(f"\nğŸ‰ è®­ç»ƒå®Œæˆï¼")
    print(f"   æœ€ç»ˆæ¨¡å‹: {model_path}")
    print(f"   æœ€ä½³æ¨¡å‹: {os.path.join(BASE_DIR, BEST_MODEL_NAME)} (éªŒè¯æŸå¤±: {best_val_loss:.6f})")
    
    # ä¿å­˜è®­ç»ƒå†å²
    import json
    history_path = os.path.join(BASE_DIR, 'training_history_full.json')
    with open(history_path, 'w') as f:
        json.dump(training_history, f, indent=2)
    print(f"   è®­ç»ƒå†å²: {history_path}")

if __name__ == "__main__":
    train()
