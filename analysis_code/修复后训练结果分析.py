"""
åˆ†æä¿®å¤åçš„è®­ç»ƒç»“æœ
è®¡ç®—å®é™…è¯¯å·®ï¼ˆè€ƒè™‘å½’ä¸€åŒ–ï¼‰

this is analysis codes about full version(80/20 split)
"""
import sys
import io
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')

import os
import json
import numpy as np
import pandas as pd

BASE_DIR = os.path.dirname(os.path.abspath(__file__))

print("="*70)
print("ä¿®å¤åè®­ç»ƒç»“æœåˆ†æ")
print("="*70)

# è¯»å–è®­ç»ƒå†å²
history_file = os.path.join(BASE_DIR, 'training_history_full.json')
if os.path.exists(history_file):
    with open(history_file, 'r', encoding='utf-8') as f:
        history = json.load(f)
    
    train_losses = history['train_loss']
    val_losses = history['val_loss']
    epochs = history['epoch']
    
    print(f"\nğŸ“Š è®­ç»ƒç»Ÿè®¡ï¼ˆå½’ä¸€åŒ–åçš„æŸå¤±ï¼‰")
    print(f"   æ€»è½®æ•°: {len(epochs)}")
    print(f"   åˆå§‹è®­ç»ƒæŸå¤±: {train_losses[0]:.6f}")
    print(f"   æœ€ç»ˆè®­ç»ƒæŸå¤±: {train_losses[-1]:.6f}")
    print(f"   æŸå¤±ä¸‹é™: {((train_losses[0] - train_losses[-1]) / train_losses[0] * 100):.2f}%")
    
    print(f"\n   åˆå§‹éªŒè¯æŸå¤±: {val_losses[0]:.6f}")
    print(f"   æœ€ç»ˆéªŒè¯æŸå¤±: {val_losses[-1]:.6f}")
    print(f"   æœ€ä½³éªŒè¯æŸå¤±: {min(val_losses):.6f}")
    print(f"   æŸå¤±ä¸‹é™: {((val_losses[0] - min(val_losses)) / val_losses[0] * 100):.2f}%")
    
    best_val_loss = min(val_losses)
    best_epoch = epochs[val_losses.index(best_val_loss)]
    print(f"\n   æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f} (ç¬¬ {best_epoch} è½®)")

# è®¡ç®—å®é™…è¯¯å·®ï¼ˆè€ƒè™‘å½’ä¸€åŒ–ï¼‰
print(f"\n" + "="*70)
print("å®é™…è¯¯å·®è®¡ç®—ï¼ˆè€ƒè™‘å½’ä¸€åŒ–ï¼‰")
print("="*70)

# éœ€è¦è®¡ç®—å½’ä¸€åŒ–å°ºåº¦
# åœ¨è®­ç»ƒæ—¶ï¼Œæ¯ä¸ªæ ·æœ¬éƒ½é™¤ä»¥äº† pointcloud çš„æ ‡å‡†å·®
# æˆ‘ä»¬éœ€è¦ä¼°ç®—å¹³å‡å°ºåº¦

# åŠ è½½ä¸€äº›æ ·æœ¬è®¡ç®—å¹³å‡å°ºåº¦
EXPORT_ROOT = os.path.join(BASE_DIR, 'data', 'pointcloud')
LABELS_FILE = os.path.join(BASE_DIR, 'labels.csv')
PROJECTS_LIST_FILE = os.path.join(BASE_DIR, 'valid_projects.txt')

if os.path.exists(PROJECTS_LIST_FILE) and os.path.exists(LABELS_FILE):
    with open(PROJECTS_LIST_FILE, 'r', encoding='utf-8') as f:
        project_names = [line.strip() for line in f if line.strip()]
    
    labels_df = pd.read_csv(LABELS_FILE, header=None)
    all_labels_np = labels_df.values.astype(np.float32)
    
    scales = []
    for i, project_name in enumerate(project_names[:20]):  # æ£€æŸ¥å‰20ä¸ªæ ·æœ¬
        project_dir = os.path.join(EXPORT_ROOT, project_name)
        pointcloud_file = os.path.join(project_dir, "pointcloud_full.npy")
        
        if os.path.exists(pointcloud_file):
            try:
                pointcloud = np.load(pointcloud_file).astype(np.float32)
                current_label = all_labels_np[i].reshape(9, 3)
                
                # ä½¿ç”¨åœ°æ ‡ç‚¹è´¨å¿ƒ
                label_centroid = np.mean(current_label, axis=0)
                centered_pointcloud = pointcloud - label_centroid
                
                # è®¡ç®—å°ºåº¦ï¼ˆæ ‡å‡†å·®ï¼‰
                scale = np.std(centered_pointcloud)
                if scale > 1e-6:
                    scales.append(scale)
            except:
                continue
    
    if scales:
        avg_scale = np.mean(scales)
        print(f"\n   ä¼°ç®—çš„å¹³å‡å½’ä¸€åŒ–å°ºåº¦: {avg_scale:.2f}")
        print(f"   å°ºåº¦èŒƒå›´: {np.min(scales):.2f} - {np.max(scales):.2f}")
        
        # è®¡ç®—å®é™…RMSE
        # å½’ä¸€åŒ–åçš„RMSE = sqrt(normalized_loss)
        # å®é™…RMSE = å½’ä¸€åŒ–åçš„RMSE Ã— å°ºåº¦
        normalized_rmse = np.sqrt(best_val_loss)
        actual_rmse = normalized_rmse * avg_scale
        
        print(f"\n   å½’ä¸€åŒ–åçš„RMSE: {normalized_rmse:.6f}")
        print(f"   å®é™…RMSE (ä¼°ç®—): {actual_rmse:.2f}")
        
        print(f"\n   è¯¯å·®åˆ†æ:")
        print(f"     ç›®æ ‡è¯¯å·®: 2mm")
        print(f"     å½“å‰è¯¯å·®: {actual_rmse:.2f}")
        
        if actual_rmse < 2:
            print(f"     âœ… è¾¾åˆ°ç›®æ ‡ï¼è¯¯å·®å°äº2mm")
        elif actual_rmse < 5:
            print(f"     âœ… æ¥è¿‘ç›®æ ‡ï¼è¯¯å·®å°äº5mm")
        elif actual_rmse < 10:
            print(f"     âš ï¸  éœ€è¦æ”¹è¿›ï¼Œä½†å·²ç»å¾ˆå¥½ï¼ˆè¯¯å·®å°äº10mmï¼‰")
        else:
            print(f"     âš ï¸  éœ€è¦è¿›ä¸€æ­¥æ”¹è¿›ï¼ˆè¯¯å·® {actual_rmse:.2f}mmï¼‰")
        
        # è®¡ç®—æ”¹è¿›å€æ•°
        old_rmse = 494.65  # ä¿®å¤å‰çš„RMSE
        improvement = old_rmse / actual_rmse
        print(f"\n   æ”¹è¿›æƒ…å†µ:")
        print(f"     ä¿®å¤å‰RMSE: {old_rmse:.2f}mm")
        print(f"     ä¿®å¤åRMSE: {actual_rmse:.2f}mm")
        print(f"     æ”¹è¿›å€æ•°: {improvement:.1f}å€")
        print(f"     æ”¹è¿›å¹…åº¦: {((old_rmse - actual_rmse) / old_rmse * 100):.1f}%")

print("\n" + "="*70)
print("è®­ç»ƒè´¨é‡åˆ†æ")
print("="*70)

if os.path.exists(history_file):
    with open(history_file, 'r', encoding='utf-8') as f:
        history = json.load(f)
    
    train_losses = history['train_loss']
    val_losses = history['val_loss']
    
    # è¿‡æ‹Ÿåˆåˆ†æ
    final_train_loss = train_losses[-1]
    final_val_loss = val_losses[-1]
    best_val_loss = min(val_losses)
    
    print(f"\n   æœ€ç»ˆè®­ç»ƒæŸå¤±: {final_train_loss:.6f}")
    print(f"   æœ€ç»ˆéªŒè¯æŸå¤±: {final_val_loss:.6f}")
    print(f"   æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}")
    
    gap = final_train_loss - final_val_loss
    print(f"   è®­ç»ƒ-éªŒè¯å·®è·: {gap:.6f}")
    
    if abs(gap) < final_val_loss * 0.1:
        print(f"   âœ… è®­ç»ƒå’ŒéªŒè¯æŸå¤±æ¥è¿‘ï¼ˆæ³›åŒ–è‰¯å¥½ï¼‰")
    elif gap < 0:
        print(f"   âš ï¸  éªŒè¯æŸå¤± > è®­ç»ƒæŸå¤±ï¼ˆå¯èƒ½è¿‡æ‹Ÿåˆï¼‰")
    else:
        print(f"   âœ… è®­ç»ƒæŸå¤±ç•¥ä½äºéªŒè¯æŸå¤±ï¼ˆæ­£å¸¸ï¼‰")
    
    # æ”¶æ•›æ€§åˆ†æ
    last_50_train = train_losses[-50:]
    last_50_val = val_losses[-50:]
    
    train_std = np.std(last_50_train)
    val_std = np.std(last_50_val)
    
    print(f"\n   æœ€å50è½®ç¨³å®šæ€§:")
    print(f"     è®­ç»ƒæŸå¤±æ ‡å‡†å·®: {train_std:.6f}")
    print(f"     éªŒè¯æŸå¤±æ ‡å‡†å·®: {val_std:.6f}")
    
    if train_std < np.mean(last_50_train) * 0.1:
        print(f"     âœ… è®­ç»ƒæŸå¤±å·²ç¨³å®š")
    if val_std < np.mean(last_50_val) * 0.1:
        print(f"     âœ… éªŒè¯æŸå¤±å·²ç¨³å®š")

print("\n" + "="*70)
print("æ€»ç»“å’Œå»ºè®®")
print("="*70)

if os.path.exists(history_file):
    with open(history_file, 'r', encoding='utf-8') as f:
        history = json.load(f)
    
    val_losses = history['val_loss']
    best_val_loss = min(val_losses)
    
    if scales:
        normalized_rmse = np.sqrt(best_val_loss)
        actual_rmse = normalized_rmse * avg_scale
        
        print(f"\nâœ… è®­ç»ƒéå¸¸æˆåŠŸï¼")
        print(f"   - æŸå¤±ä» {history['train_loss'][0]:.6f} é™åˆ° {best_val_loss:.6f}")
        print(f"   - æ”¹è¿›å¹…åº¦: {((history['train_loss'][0] - best_val_loss) / history['train_loss'][0] * 100):.2f}%")
        print(f"   - ä¼°ç®—å®é™…RMSE: {actual_rmse:.2f}mm")
        
        if actual_rmse < 2:
            print(f"\nğŸ‰ æ­å–œï¼å·²è¾¾åˆ°2mmç›®æ ‡ï¼")
        elif actual_rmse < 5:
            print(f"\nâœ… éå¸¸æ¥è¿‘ç›®æ ‡ï¼è¯¯å·®å°äº5mm")
            print(f"   å»ºè®®:")
            print(f"     1. å¯ä»¥å°è¯•å¢åŠ æ­£åˆ™åŒ–è¿›ä¸€æ­¥å‡å°‘è¿‡æ‹Ÿåˆ")
            print(f"     2. ä½¿ç”¨æ›´å¤šè®­ç»ƒæ•°æ®")
            print(f"     3. æ•°æ®å¢å¼º")
        else:
            print(f"\nğŸ’¡ éœ€è¦è¿›ä¸€æ­¥æ”¹è¿›:")
            print(f"   1. å¢åŠ æ­£åˆ™åŒ–ï¼ˆDropout, æƒé‡è¡°å‡ï¼‰")
            print(f"   2. ä½¿ç”¨æ›´å¤šè®­ç»ƒæ•°æ®")
            print(f"   3. æ•°æ®å¢å¼º")
            print(f"   4. æ”¹è¿›æ¨¡å‹æ¶æ„")

print("="*70)
