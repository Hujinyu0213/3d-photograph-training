"""
åœ¨KæŠ˜äº¤å‰éªŒè¯çš„æµ‹è¯•é›†ä¸Šè¯„ä¼°å•æ¬¡è®­ç»ƒæ¨¡å‹
ä½¿ç”¨ä¸KæŠ˜æ¨¡å‹ç›¸åŒçš„æµ‹è¯•é›†ï¼ˆ10ä¸ªæ ·æœ¬ï¼Œ90/10åˆ’åˆ†ï¼Œéšæœºç§å­42ï¼‰
ç¡®ä¿å…¬å¹³å¯¹æ¯”
"""
import os
import sys
import io
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')

# æ·»åŠ å½“å‰ç›®å½•åˆ° Python è·¯å¾„
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
if BASE_DIR not in sys.path:
    sys.path.insert(0, BASE_DIR)

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import TensorDataset, DataLoader, random_split
from tqdm import tqdm
from pointnet_utils import PointNetEncoder, feature_transform_reguliarzer
import json

# =========================================================
# é…ç½®
# =========================================================
EXPORT_ROOT = os.path.join(BASE_DIR, 'data', 'pointcloud')
LABELS_FILE = os.path.join(BASE_DIR, 'labels.csv')
PROJECTS_LIST_FILE = os.path.join(BASE_DIR, 'valid_projects.txt')

NUM_TARGET_POINTS = 9
OUTPUT_DIM = NUM_TARGET_POINTS * 3  # 27ç»´

# æ¨¡å‹æ–‡ä»¶ï¼ˆå•æ¬¡è®­ç»ƒæ¨¡å‹ï¼‰
MODEL_PATH = os.path.join(BASE_DIR, 'pointnet_regression_model_full_best.pth')

# GPU é…ç½®
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}")
if torch.cuda.is_available():
    print(f"   GPUåç§°: {torch.cuda.get_device_name(0)}")

# =========================================================
# æ¨¡å‹å®šä¹‰ï¼ˆä¸è®­ç»ƒæ—¶ç›¸åŒï¼‰
# =========================================================
class PointNetRegressor(nn.Module):
    def __init__(self, output_dim=27, dropout_rate=0.3):
        super(PointNetRegressor, self).__init__()
        self.feat = PointNetEncoder(global_feat=True, feature_transform=True, channel=3)
        self.fc1 = nn.Linear(1024, 512)
        self.fc2 = nn.Linear(512, 256)
        self.fc3 = nn.Linear(256, output_dim)
        self.bn1 = nn.BatchNorm1d(512)
        self.bn2 = nn.BatchNorm1d(256)
        self.dropout = nn.Dropout(p=dropout_rate)
        self.relu = nn.ReLU()

    def forward(self, x):
        x, trans, trans_feat = self.feat(x)
        x = self.relu(self.bn1(self.fc1(x)))
        x = self.relu(self.bn2(self.dropout(self.fc2(x))))
        x = self.fc3(x)
        return x, trans_feat

# =========================================================
# æ•°æ®åŠ è½½ï¼ˆä¸è®­ç»ƒæ—¶ç›¸åŒï¼‰
# =========================================================
def load_data():
    """åŠ è½½å®Œæ•´ç‚¹äº‘å’Œæ ‡ç­¾ï¼Œä½¿ç”¨ä¸è®­ç»ƒæ—¶ç›¸åŒçš„é¢„å¤„ç†"""
    print("--- æ­£åœ¨åŠ è½½å®Œæ•´ç‚¹äº‘å’Œæ ‡ç­¾ ---")
    
    if not os.path.exists(PROJECTS_LIST_FILE):
        raise FileNotFoundError(f"âŒ é¡¹ç›®åˆ—è¡¨æ–‡ä»¶ä¸å­˜åœ¨: {PROJECTS_LIST_FILE}")
    
    with open(PROJECTS_LIST_FILE, 'r', encoding='utf-8') as f:
        project_names = [line.strip() for line in f if line.strip()]
    
    if not os.path.exists(LABELS_FILE):
        raise FileNotFoundError(f"âŒ æ ‡ç­¾æ–‡ä»¶ä¸å­˜åœ¨: {LABELS_FILE}")
    
    labels_df = pd.read_csv(LABELS_FILE, header=None)
    all_labels_np = labels_df.values.astype(np.float32)
    
    if len(project_names) != len(all_labels_np):
        min_len = min(len(project_names), len(all_labels_np))
        project_names = project_names[:min_len]
        all_labels_np = all_labels_np[:min_len]
    
    valid_features = []
    valid_labels = []
    point_counts = []
    scales_list = []  # ä¿å­˜æ¯ä¸ªæ ·æœ¬çš„å½’ä¸€åŒ–å°ºåº¦
    label_centroids_list = []  # ä¿å­˜æ¯ä¸ªæ ·æœ¬çš„åœ°æ ‡ç‚¹è´¨å¿ƒ
    
    print(f"åŠ è½½ {len(project_names)} ä¸ªæ ·æœ¬çš„ç‚¹äº‘...")
    
    for i, project_name in enumerate(tqdm(project_names, desc="åŠ è½½ç‚¹äº‘")):
        project_dir = os.path.join(EXPORT_ROOT, project_name)
        pointcloud_file = os.path.join(project_dir, "pointcloud_full.npy")
        
        if not os.path.exists(pointcloud_file):
            continue
        
        try:
            pointcloud = np.load(pointcloud_file).astype(np.float32)
            
            if len(pointcloud) == 0:
                continue
            
            # ä½¿ç”¨ä¸è®­ç»ƒæ—¶ç›¸åŒçš„é¢„å¤„ç†
            current_label = all_labels_np[i].reshape(NUM_TARGET_POINTS, 3)
            label_centroid = np.mean(current_label, axis=0)
            centered_pointcloud = pointcloud - label_centroid
            centered_label = current_label - label_centroid
            
            # å½’ä¸€åŒ–
            scale = np.std(centered_pointcloud)
            if scale > 1e-6:
                centered_pointcloud = centered_pointcloud / scale
                centered_label = centered_label / scale
            else:
                scale = 1.0
            
            scales_list.append(scale)
            label_centroids_list.append(label_centroid)
            
            # è½¬ç½®ä¸º (3, N) æ ¼å¼
            centered_pointcloud_T = centered_pointcloud.T
            
            valid_features.append(centered_pointcloud_T)
            valid_labels.append(centered_label.flatten())
            point_counts.append(len(pointcloud))
            
        except Exception as e:
            continue
    
    if not valid_features:
        raise RuntimeError("âŒ æœªèƒ½åŠ è½½ä»»ä½•æœ‰æ•ˆæ•°æ®ï¼")
    
    print(f"\nâœ… æˆåŠŸåŠ è½½ {len(valid_features)} ä¸ªæ ·æœ¬")
    
    # ç»Ÿä¸€é‡‡æ ·åˆ°å›ºå®šç‚¹æ•°
    MAX_POINTS = 8192
    print(f"ç»Ÿä¸€é‡‡æ ·åˆ° {MAX_POINTS} ä¸ªç‚¹...")
    
    processed_features = []
    for feat in valid_features:
        num_points = feat.shape[1]
        if num_points >= MAX_POINTS:
            indices = np.random.choice(num_points, MAX_POINTS, replace=False)
            sampled_feat = feat[:, indices]
        else:
            indices = np.random.choice(num_points, MAX_POINTS, replace=True)
            sampled_feat = feat[:, indices]
        processed_features.append(sampled_feat)
    
    X_np = np.array(processed_features, dtype=np.float32)
    Y_np = np.array(valid_labels, dtype=np.float32)
    
    print(f"   æœ€ç»ˆæ•°æ®å½¢çŠ¶: X={X_np.shape}, Y={Y_np.shape}")
    
    return torch.from_numpy(X_np), torch.from_numpy(Y_np), scales_list, label_centroids_list

# =========================================================
# è¯„ä¼°å‡½æ•°
# =========================================================
def evaluate_single_model_on_kfold_testset():
    """åœ¨KæŠ˜äº¤å‰éªŒè¯çš„æµ‹è¯•é›†ä¸Šè¯„ä¼°å•æ¬¡è®­ç»ƒæ¨¡å‹"""
    print("\n" + "="*70)
    print("åœ¨KæŠ˜æµ‹è¯•é›†ä¸Šè¯„ä¼°å•æ¬¡è®­ç»ƒæ¨¡å‹ï¼ˆå…¬å¹³å¯¹æ¯”ï¼‰")
    print("="*70)
    
    # åŠ è½½æ•°æ®
    X, Y, scales_list, label_centroids_list = load_data()
    
    # åˆ’åˆ†æµ‹è¯•é›†ï¼ˆä¸KæŠ˜è®­ç»ƒæ—¶ç›¸åŒçš„æ¯”ä¾‹å’Œéšæœºç§å­ï¼‰
    TEST_RATIO = 0.1  # 10%ä½œä¸ºæµ‹è¯•é›†ï¼ˆä¸KæŠ˜è®­ç»ƒæ—¶ç›¸åŒï¼‰
    dataset = TensorDataset(X, Y)
    test_size = int(TEST_RATIO * len(dataset))
    train_val_size = len(dataset) - test_size
    
    # ä½¿ç”¨ä¸KæŠ˜è®­ç»ƒæ—¶ç›¸åŒçš„éšæœºç§å­ï¼ˆ42ï¼‰
    train_val_dataset, test_dataset = random_split(
        dataset, [train_val_size, test_size],
        generator=torch.Generator().manual_seed(42)
    )
    
    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)
    
    print(f"\nğŸ“Š æ•°æ®é›†åˆ’åˆ†ï¼ˆä¸KæŠ˜è®­ç»ƒæ—¶ç›¸åŒï¼‰:")
    print(f"   æµ‹è¯•é›†: {len(test_dataset)} ä¸ªæ ·æœ¬ (10%)")
    print(f"   è®­ç»ƒ+éªŒè¯é›†: {len(train_val_dataset)} ä¸ªæ ·æœ¬ (90%)")
    print(f"   âš ï¸  æ³¨æ„ï¼šè¿™æ˜¯KæŠ˜æ¨¡å‹çš„æµ‹è¯•é›†ï¼Œç”¨äºå…¬å¹³å¯¹æ¯”")
    
    # åŠ è½½æ¨¡å‹
    print(f"\nğŸ“¦ åŠ è½½å•æ¬¡è®­ç»ƒæ¨¡å‹: {MODEL_PATH}")
    if not os.path.exists(MODEL_PATH):
        raise FileNotFoundError(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {MODEL_PATH}")
    
    model = PointNetRegressor(output_dim=OUTPUT_DIM, dropout_rate=0.3).to(device)
    model.load_state_dict(torch.load(MODEL_PATH, map_location=device))
    model.eval()
    print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    
    # è¯„ä¼°
    print(f"\nğŸ” å¼€å§‹è¯„ä¼°...")
    all_predictions = []
    all_targets = []
    all_scales = []
    all_centroids = []
    
    # é¢„å…ˆè·å–æµ‹è¯•é›†çš„æ‰€æœ‰ç´¢å¼•
    if hasattr(test_dataset.indices, 'tolist'):
        test_indices_list = test_dataset.indices.tolist()
    else:
        test_indices_list = list(test_dataset.indices)
    
    with torch.no_grad():
        sample_idx = 0  # å½“å‰å¤„ç†çš„æ ·æœ¬ç´¢å¼•ï¼ˆåœ¨æµ‹è¯•é›†ä¸­çš„ä½ç½®ï¼‰
        for batch_idx, (data, target) in enumerate(tqdm(test_loader, desc="è¯„ä¼°")):
            data, target = data.to(device), target.to(device)
            pred, _ = model(data)
            
            # è½¬æ¢ä¸ºnumpy
            pred_np = pred.cpu().numpy()
            target_np = target.cpu().numpy()
            batch_size = len(pred_np)
            
            all_predictions.append(pred_np)
            all_targets.append(target_np)
            
            # è·å–å¯¹åº”çš„å°ºåº¦å’Œè´¨å¿ƒ
            for i in range(batch_size):
                if sample_idx < len(test_indices_list):
                    orig_idx = test_indices_list[sample_idx]
                    all_scales.append(scales_list[orig_idx])
                    all_centroids.append(label_centroids_list[orig_idx])
                    sample_idx += 1
    
    # åˆå¹¶æ‰€æœ‰é¢„æµ‹å’Œæ ‡ç­¾
    all_predictions = np.concatenate(all_predictions, axis=0)
    all_targets = np.concatenate(all_targets, axis=0)
    
    print(f"\nâœ… è¯„ä¼°å®Œæˆ")
    print(f"   æµ‹è¯•æ ·æœ¬æ•°: {len(all_predictions)}")
    
    # åå½’ä¸€åŒ–ï¼ˆæ¢å¤åˆ°åŸå§‹åæ ‡å•ä½ï¼‰
    print(f"\nğŸ”„ åå½’ä¸€åŒ–é¢„æµ‹ç»“æœ...")
    predictions_denorm = []
    targets_denorm = []
    
    for i in range(len(all_predictions)):
        scale = all_scales[i]
        centroid = all_centroids[i]
        pred_norm = all_predictions[i].reshape(9, 3)
        target_norm = all_targets[i].reshape(9, 3)
        
        # åå½’ä¸€åŒ–ï¼šå…ˆä¹˜ä»¥å°ºåº¦ï¼Œå†åŠ å›è´¨å¿ƒ
        pred_denorm = pred_norm * scale + centroid
        target_denorm = target_norm * scale + centroid
        
        predictions_denorm.append(pred_denorm)
        targets_denorm.append(target_denorm)
    
    predictions_denorm = np.array(predictions_denorm)  # (N, 9, 3)
    targets_denorm = np.array(targets_denorm)  # (N, 9, 3)
    
    # è®¡ç®—è¯¯å·®
    print(f"\nğŸ“Š è®¡ç®—è¯¯å·®ç»Ÿè®¡...")
    errors = predictions_denorm - targets_denorm  # (N, 9, 3)
    
    # æ¯ä¸ªåæ ‡çš„è¯¯å·®
    coord_errors = errors.reshape(-1, 27)  # (N, 27)
    
    # æ¯ä¸ªç‚¹çš„3Dè¯¯å·®
    point_errors_3d = np.linalg.norm(errors, axis=2)  # (N, 9)
    
    # æ¯ä¸ªåæ ‡çš„RMSEå’ŒMAE
    coord_rmse = np.sqrt(np.mean(coord_errors**2, axis=0))  # (27,)
    coord_mae = np.mean(np.abs(coord_errors), axis=0)  # (27,)
    
    # æ¯ä¸ªç‚¹çš„RMSEå’ŒMAEï¼ˆ3Dè·ç¦»ï¼‰
    point_rmse_3d = np.sqrt(np.mean(point_errors_3d**2, axis=0))  # (9,)
    point_mae_3d = np.mean(point_errors_3d, axis=0)  # (9,)
    
    # æ¯ä¸ªåæ ‡çš„è¯¯å·®ï¼ˆæŒ‰X, Y, Zåˆ†åˆ«ï¼‰
    errors_xyz = errors  # (N, 9, 3)
    rmse_x = np.sqrt(np.mean(errors_xyz[:, :, 0]**2, axis=0))  # (9,)
    rmse_y = np.sqrt(np.mean(errors_xyz[:, :, 1]**2, axis=0))  # (9,)
    rmse_z = np.sqrt(np.mean(errors_xyz[:, :, 2]**2, axis=0))  # (9,)
    mae_x = np.mean(np.abs(errors_xyz[:, :, 0]), axis=0)  # (9,)
    mae_y = np.mean(np.abs(errors_xyz[:, :, 1]), axis=0)  # (9,)
    mae_z = np.mean(np.abs(errors_xyz[:, :, 2]), axis=0)  # (9,)
    
    # æ‰“å°è¯¦ç»†åˆ†æ
    print("\n" + "="*70)
    print("è¯¦ç»†åˆ†æï¼šæ‰€æœ‰9ä¸ªåœ°æ ‡ç‚¹çš„æ€§èƒ½")
    print("="*70)
    
    # æ€»ä½“ç»Ÿè®¡
    overall_rmse = np.sqrt(np.mean(coord_errors**2))
    overall_mae = np.mean(np.abs(coord_errors))
    overall_rmse_3d = np.sqrt(np.mean(point_errors_3d**2))
    overall_mae_3d = np.mean(point_errors_3d)
    
    # è®¡ç®—å¹³å‡ç²¾åº¦ï¼ˆåŸºäºæ¬§æ°è·ç¦»ï¼Œä½¿ç”¨ä¸åŒé˜ˆå€¼ï¼‰
    thresholds = [1.0, 2.0, 5.0, 10.0]  # mm
    mean_precision = {}
    for threshold in thresholds:
        precision_per_point = []
        for point_idx in range(9):
            errors_point = point_errors_3d[:, point_idx]
            precision = np.sum(errors_point < threshold) / len(errors_point) * 100
            precision_per_point.append(precision)
        mean_precision[threshold] = np.mean(precision_per_point)
    
    print(f"\nğŸ“Š æ€»ä½“æ€§èƒ½:")
    print(f"   æ‰€æœ‰åæ ‡çš„RMSE: {overall_rmse:.4f}")
    print(f"   æ‰€æœ‰åæ ‡çš„MAE: {overall_mae:.4f}")
    print(f"   æ‰€æœ‰ç‚¹çš„3D RMSE: {overall_rmse_3d:.4f}")
    print(f"   æ‰€æœ‰ç‚¹çš„3D MAE: {overall_mae_3d:.4f}")
    print(f"\nğŸ“Š å¹³å‡ç²¾åº¦ï¼ˆåŸºäºæ¬§æ°è·ç¦»ï¼‰:")
    for threshold in thresholds:
        print(f"   å¹³å‡ç²¾åº¦ @ {threshold}mm: {mean_precision[threshold]:.2f}%")
    
    # ä¿å­˜ç»“æœ
    results = {
        'overall': {
            'rmse_all_coords': float(overall_rmse),
            'mae_all_coords': float(overall_mae),
            'rmse_3d_all_points': float(overall_rmse_3d),
            'mae_3d_all_points': float(overall_mae_3d),
            'mean_precision_1mm': float(mean_precision[1.0]),
            'mean_precision_2mm': float(mean_precision[2.0]),
            'mean_precision_5mm': float(mean_precision[5.0]),
            'mean_precision_10mm': float(mean_precision[10.0])
        },
        'points': []
    }
    
    # åœ°æ ‡ç‚¹åç§°
    landmark_names = ['Glabella', 'Nasion', 'Rhinion', 'Nasal Tip', 'Subnasale', 
                      'Alare (R)', 'Alare (L)', 'Zygion (R)', 'Zygion (L)']
    
    for point_idx in range(9):
        errors_point = point_errors_3d[:, point_idx]
        point_results = {
            'point_id': point_idx + 1,
            'landmark_name': landmark_names[point_idx],
            'rmse_3d': float(point_rmse_3d[point_idx]),
            'mae_3d': float(point_mae_3d[point_idx]),
            'rmse_x': float(rmse_x[point_idx]),
            'rmse_y': float(rmse_y[point_idx]),
            'rmse_z': float(rmse_z[point_idx]),
            'mae_x': float(mae_x[point_idx]),
            'mae_y': float(mae_y[point_idx]),
            'mae_z': float(mae_z[point_idx]),
            'max_error': float(np.max(point_errors_3d[:, point_idx])),
            'min_error': float(np.min(point_errors_3d[:, point_idx])),
            'median_error': float(np.median(point_errors_3d[:, point_idx])),
            'std_error': float(np.std(point_errors_3d[:, point_idx])),
            'precision_1mm': float(np.sum(errors_point < 1) / len(errors_point) * 100),
            'precision_2mm': float(np.sum(errors_point < 2) / len(errors_point) * 100),
            'precision_5mm': float(np.sum(errors_point < 5) / len(errors_point) * 100),
            'precision_10mm': float(np.sum(errors_point < 10) / len(errors_point) * 100)
        }
        results['points'].append(point_results)
    
    results_file = os.path.join(BASE_DIR, 'single_model_on_kfold_testset_results.json')
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2)
    
    print(f"\nâœ… ç»“æœå·²ä¿å­˜: {results_file}")
    
    # æ€»ç»“
    print(f"\n" + "="*70)
    print("æ€»ç»“")
    print("="*70)
    
    points_under_2mm = np.sum(point_rmse_3d < 2)
    print(f"\nè¾¾åˆ°2mmç›®æ ‡çš„ç‚¹æ•°: {points_under_2mm}/9 ({points_under_2mm/9*100:.1f}%)")
    
    avg_rmse = np.mean(point_rmse_3d)
    print(f"å¹³å‡3D RMSE: {avg_rmse:.4f} mm")
    
    print(f"\nğŸ“Š å¹³å‡ç²¾åº¦ï¼ˆåŸºäºæ¬§æ°è·ç¦»ï¼‰:")
    print(f"   å¹³å‡ç²¾åº¦ @ 1mm: {mean_precision[1.0]:.2f}%")
    print(f"   å¹³å‡ç²¾åº¦ @ 2mm: {mean_precision[2.0]:.2f}%")
    print(f"   å¹³å‡ç²¾åº¦ @ 5mm: {mean_precision[5.0]:.2f}%")
    print(f"   å¹³å‡ç²¾åº¦ @ 10mm: {mean_precision[10.0]:.2f}%")
    
    if avg_rmse < 2:
        print(f"âœ… æ€»ä½“æ€§èƒ½ä¼˜ç§€ï¼å¹³å‡è¯¯å·®å°äº2mm")
    elif avg_rmse < 5:
        print(f"âš ï¸  æ€»ä½“æ€§èƒ½è‰¯å¥½ï¼Œä½†éœ€è¦è¿›ä¸€æ­¥æ”¹è¿›")
    else:
        print(f"âŒ éœ€è¦æ˜¾è‘—æ”¹è¿›")
    
    if mean_precision[2.0] >= 80:
        print(f"âœ… å¹³å‡ç²¾åº¦ä¼˜ç§€ï¼2mmç²¾åº¦è¾¾åˆ°{mean_precision[2.0]:.1f}%")
    elif mean_precision[2.0] >= 50:
        print(f"âš ï¸  å¹³å‡ç²¾åº¦è‰¯å¥½ï¼Œ2mmç²¾åº¦ä¸º{mean_precision[2.0]:.1f}%ï¼Œéœ€è¦æ”¹è¿›")
    else:
        print(f"âŒ å¹³å‡ç²¾åº¦è¾ƒä½ï¼Œ2mmç²¾åº¦ä»…ä¸º{mean_precision[2.0]:.1f}%ï¼Œéœ€è¦æ˜¾è‘—æ”¹è¿›")
    
    print("="*70)
    
    return results

if __name__ == "__main__":
    evaluate_single_model_on_kfold_testset()
