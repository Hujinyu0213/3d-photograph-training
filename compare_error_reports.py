"""
è¯¯å·®æŠ¥å‘Šæ¯”è¾ƒå·¥å…·
ç”¨äºŽæ¯”è¾ƒä¸¤ä¸ª error_report.csv æ–‡ä»¶çš„å·®å¼‚
"""
import os
import sys
import pandas as pd
import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt

# è®¾ç½® matplotlib ä½¿ç”¨æ”¯æŒä¸­æ–‡çš„å­—ä½“ï¼Œå¦‚æžœå¤±è´¥åˆ™ä½¿ç”¨è‹±æ–‡
try:
    plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'Arial Unicode MS', 'DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False
except:
    # å¦‚æžœä¸­æ–‡å­—ä½“ä¸å¯ç”¨ï¼Œä½¿ç”¨è‹±æ–‡æ ‡ç­¾
    pass

# æ·»åŠ å½“å‰ç›®å½•åˆ° Python è·¯å¾„
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
if BASE_DIR not in sys.path:
    sys.path.insert(0, BASE_DIR)

OUTPUT_DIR = os.path.join(BASE_DIR, 'results')

def compare_error_reports(report1_path, report2_path, name1="æŠ¥å‘Š1", name2="æŠ¥å‘Š2"):
    """
    æ¯”è¾ƒä¸¤ä¸ªè¯¯å·®æŠ¥å‘Š
    
    å‚æ•°:
        report1_path: ç¬¬ä¸€ä¸ªæŠ¥å‘Šæ–‡ä»¶è·¯å¾„
        report2_path: ç¬¬äºŒä¸ªæŠ¥å‘Šæ–‡ä»¶è·¯å¾„
        name1: ç¬¬ä¸€ä¸ªæŠ¥å‘Šçš„åç§°
        name2: ç¬¬äºŒä¸ªæŠ¥å‘Šçš„åç§°
    """
    print("=" * 60)
    print("ðŸ“Š è¯¯å·®æŠ¥å‘Šæ¯”è¾ƒå·¥å…·")
    print("=" * 60)
    
    # è¯»å–æŠ¥å‘Š
    try:
        df1 = pd.read_csv(report1_path)
        print(f"\nâœ… æˆåŠŸåŠ è½½ {name1}: {report1_path}")
        print(f"   æ ·æœ¬æ•°: {len(df1)}")
    except Exception as e:
        print(f"âŒ æ— æ³•åŠ è½½ {name1}: {e}")
        return
    
    try:
        df2 = pd.read_csv(report2_path)
        print(f"âœ… æˆåŠŸåŠ è½½ {name2}: {report2_path}")
        print(f"   æ ·æœ¬æ•°: {len(df2)}")
    except Exception as e:
        print(f"âŒ æ— æ³•åŠ è½½ {name2}: {e}")
        return
    
    # æ£€æŸ¥åˆ—æ˜¯å¦ä¸€è‡´
    if list(df1.columns) != list(df2.columns):
        print("âš ï¸  è­¦å‘Š: ä¸¤ä¸ªæŠ¥å‘Šçš„åˆ—ä¸å®Œå…¨ä¸€è‡´")
        common_cols = set(df1.columns) & set(df2.columns)
        print(f"   å…±åŒåˆ—: {len(common_cols)} ä¸ª")
        df1 = df1[list(common_cols)]
        df2 = df2[list(common_cols)]
    
    # ç¡®ä¿æ ·æœ¬æ•°ä¸€è‡´
    min_samples = min(len(df1), len(df2))
    df1 = df1.iloc[:min_samples]
    df2 = df2.iloc[:min_samples]
    print(f"\nðŸ“Š æ¯”è¾ƒ {min_samples} ä¸ªæ ·æœ¬")
    
    # æ¯”è¾ƒä¸»è¦æŒ‡æ ‡
    print("\n" + "=" * 60)
    print("ðŸ“ˆ ä¸»è¦æŒ‡æ ‡æ¯”è¾ƒ")
    print("=" * 60)
    
    metrics = ['RMSE', 'MAE', 'X_Error', 'Y_Error', 'Z_Error']
    available_metrics = [m for m in metrics if m in df1.columns]
    
    comparison_results = {}
    
    for metric in available_metrics:
        val1 = df1[metric].values
        val2 = df2[metric].values
        
        mean1 = np.mean(val1)
        mean2 = np.mean(val2)
        std1 = np.std(val1)
        std2 = np.std(val2)
        min1 = np.min(val1)
        min2 = np.min(val2)
        max1 = np.max(val1)
        max2 = np.max(val2)
        
        diff = mean2 - mean1
        improvement = (mean1 - mean2) / mean1 * 100 if mean1 > 0 else 0
        
        comparison_results[metric] = {
            'mean1': mean1, 'mean2': mean2, 'diff': diff, 'improvement': improvement
        }
        
        print(f"\n{metric}:")
        print(f"  {name1:15s}: {mean1:.4f} Â± {std1:.4f} (èŒƒå›´: {min1:.4f} - {max1:.4f})")
        print(f"  {name2:15s}: {mean2:.4f} Â± {std2:.4f} (èŒƒå›´: {min2:.4f} - {max2:.4f})")
        
        if diff < 0:
            print(f"  âœ… {name2} æ›´å¥½ï¼ˆä½Ž {abs(diff):.4f}ï¼Œæ”¹å–„ {abs(improvement):.2f}%ï¼‰")
        elif diff > 0:
            print(f"  âœ… {name1} æ›´å¥½ï¼ˆä½Ž {diff:.4f}ï¼Œæ”¹å–„ {improvement:.2f}%ï¼‰")
        else:
            print(f"  ðŸ¤ ä¸¤ä¸ªæŠ¥å‘Šç›¸åŒ")
    
    # æ¯”è¾ƒæ¯ä¸ªåœ°æ ‡ç‚¹çš„è¯¯å·®
    landmark_cols = [col for col in df1.columns if col.endswith('_Error') and col not in metrics]
    
    if landmark_cols:
        print("\n" + "=" * 60)
        print("ðŸ“ å„åœ°æ ‡ç‚¹è¯¯å·®æ¯”è¾ƒ")
        print("=" * 60)
        
        landmark_comparison = []
        for col in landmark_cols:
            landmark_name = col.replace('_Error', '')
            mean1 = np.mean(df1[col].values)
            mean2 = np.mean(df2[col].values)
            diff = mean2 - mean1
            improvement = (mean1 - mean2) / mean1 * 100 if mean1 > 0 else 0
            
            landmark_comparison.append({
                'landmark': landmark_name,
                'mean1': mean1,
                'mean2': mean2,
                'diff': diff,
                'improvement': improvement
            })
            
            status = "âœ…" if diff < 0 else "âš ï¸" if diff > 0 else "ðŸ¤"
            print(f"{status} {landmark_name:20s}: {name1}={mean1:.4f}, {name2}={mean2:.4f}, "
                  f"å·®å¼‚={diff:+.4f} ({improvement:+.2f}%)")
    
    # ç»˜åˆ¶æ¯”è¾ƒå›¾è¡¨
    print("\n" + "=" * 60)
    print("ðŸ“Š ç”Ÿæˆæ¯”è¾ƒå›¾è¡¨...")
    print("=" * 60)
    
    # åˆ›å»ºå›¾è¡¨
    n_metrics = len(available_metrics)
    if n_metrics > 0:
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        axes = axes.flatten()
        
        for idx, metric in enumerate(available_metrics[:4]):  # æœ€å¤šæ˜¾ç¤º4ä¸ªæŒ‡æ ‡
            ax = axes[idx]
            
            data1 = df1[metric].values
            data2 = df2[metric].values
            
            bp = ax.boxplot([data1, data2], labels=[name1, name2], patch_artist=True)
            
            # è®¾ç½®é¢œè‰²
            colors = ['lightblue', 'lightcoral']
            for patch, color in zip(bp['boxes'], colors):
                patch.set_facecolor(color)
            
            ax.set_ylabel(f'{metric} (mm)', fontsize=10)
            ax.set_title(f'{metric} Comparison', fontsize=11)
            ax.grid(True, alpha=0.3)
        
        # éšè—å¤šä½™çš„å­å›¾
        for idx in range(len(available_metrics), 4):
            axes[idx].axis('off')
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        comparison_path = os.path.join(OUTPUT_DIR, 'error_report_comparison.png')
        plt.savefig(comparison_path, dpi=150, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… æ¯”è¾ƒå›¾è¡¨å·²ä¿å­˜åˆ°: {comparison_path}")
    
    # ä¿å­˜è¯¦ç»†æ¯”è¾ƒæŠ¥å‘Š
    comparison_df = pd.DataFrame(comparison_results).T
    comparison_df.columns = [f'{name1}_Mean', f'{name2}_Mean', 'Difference', 'Improvement_%']
    comparison_df.index.name = 'Metric'
    
    comparison_csv_path = os.path.join(OUTPUT_DIR, 'error_report_comparison.csv')
    comparison_df.to_csv(comparison_csv_path)
    print(f"âœ… è¯¦ç»†æ¯”è¾ƒæŠ¥å‘Šå·²ä¿å­˜åˆ°: {comparison_csv_path}")
    
    # æ€»ç»“
    print("\n" + "=" * 60)
    print("ðŸ“‹ æ€»ç»“")
    print("=" * 60)
    
    if 'RMSE' in comparison_results:
        rmse_improvement = comparison_results['RMSE']['improvement']
        if rmse_improvement > 0:
            print(f"âœ… {name2} çš„ RMSE æ¯” {name1} æ”¹å–„äº† {rmse_improvement:.2f}%")
        elif rmse_improvement < 0:
            print(f"âš ï¸  {name2} çš„ RMSE æ¯” {name1} å·®äº† {abs(rmse_improvement):.2f}%")
        else:
            print(f"ðŸ¤ ä¸¤ä¸ªæŠ¥å‘Šçš„ RMSE ç›¸åŒ")
    
    if 'MAE' in comparison_results:
        mae_improvement = comparison_results['MAE']['improvement']
        if mae_improvement > 0:
            print(f"âœ… {name2} çš„ MAE æ¯” {name1} æ”¹å–„äº† {mae_improvement:.2f}%")
        elif mae_improvement < 0:
            print(f"âš ï¸  {name2} çš„ MAE æ¯” {name1} å·®äº† {abs(mae_improvement):.2f}%")
        else:
            print(f"ðŸ¤ ä¸¤ä¸ªæŠ¥å‘Šçš„ MAE ç›¸åŒ")

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description='æ¯”è¾ƒä¸¤ä¸ªè¯¯å·®æŠ¥å‘Š')
    parser.add_argument('report1', help='ç¬¬ä¸€ä¸ªè¯¯å·®æŠ¥å‘Šæ–‡ä»¶è·¯å¾„')
    parser.add_argument('report2', help='ç¬¬äºŒä¸ªè¯¯å·®æŠ¥å‘Šæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--name1', default='æŠ¥å‘Š1', help='ç¬¬ä¸€ä¸ªæŠ¥å‘Šçš„åç§°')
    parser.add_argument('--name2', default='æŠ¥å‘Š2', help='ç¬¬äºŒä¸ªæŠ¥å‘Šçš„åç§°')
    
    args = parser.parse_args()
    
    compare_error_reports(args.report1, args.report2, args.name1, args.name2)

