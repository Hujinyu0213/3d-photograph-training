import os
import sys

# æ·»åŠ å½“å‰ç›®å½•åˆ° Python è·¯å¾„ï¼Œç¡®ä¿èƒ½æ‰¾åˆ° pointnet_utils æ¨¡å—
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
if BASE_DIR not in sys.path:
    sys.path.insert(0, BASE_DIR)

os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
import torch
import numpy as np
import matplotlib
matplotlib.use('Agg')  # ä½¿ç”¨éäº¤äº’å¼åç«¯ï¼Œé¿å…æ˜¾ç¤ºè­¦å‘Š
import matplotlib.pyplot as plt
import pandas as pd
import torch.nn as nn
from pointnet_utils import PointNetEncoder

class PointNetRegressor(nn.Module):
    def __init__(self, output_dim=27):
        super(PointNetRegressor, self).__init__()
        self.feat = PointNetEncoder(global_feat=True, feature_transform=True, channel=3)
        self.fc1 = nn.Linear(1024, 512)
        self.fc2 = nn.Linear(512, 256)
        self.fc3 = nn.Linear(256, output_dim) 
        self.bn1 = nn.BatchNorm1d(512)
        self.bn2 = nn.BatchNorm1d(256)
        self.dropout = nn.Dropout(p=0.4)
        self.relu = nn.ReLU()

    def forward(self, x):
        x, trans, trans_feat = self.feat(x)
        x = self.relu(self.bn1(self.fc1(x)))
        x = self.relu(self.bn2(self.dropout(self.fc2(x))))
        x = self.fc3(x) 
        return x, trans_feat

# é…ç½®ï¼ˆBASE_DIR å·²åœ¨ä¸Šé¢å®šä¹‰ï¼‰
DATA_DIR = os.path.join(BASE_DIR, 'data') 
LABELS_FILE = os.path.join(BASE_DIR, 'labels.csv')
DEFAULT_MODEL_PATH = os.path.join(BASE_DIR, 'pointnet_regression_model.pth')  # é»˜è®¤æ¨¡å‹è·¯å¾„
OUTPUT_DIR = os.path.join(BASE_DIR, 'results')  # ä¿å­˜å›¾ç‰‡çš„ç›®å½•

def visualize_result(evaluate_all=False, model_path=None):
    """
    è¯„ä¼°æ¨¡å‹æ€§èƒ½
    
    å‚æ•°:
        evaluate_all: å¦‚æœä¸ºTrueï¼Œè¯„ä¼°æ‰€æœ‰æ ·æœ¬ï¼›å¦‚æœä¸ºFalseï¼Œåªè¯„ä¼°æ ·æœ¬1, 11, 51
        model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„
    """
    # ç¡®å®šä½¿ç”¨çš„æ¨¡å‹è·¯å¾„
    if model_path is None:
        model_path = DEFAULT_MODEL_PATH
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}")
    model = PointNetRegressor(output_dim=27).to(device)
    
    if not os.path.exists(model_path):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶ {model_path}")
        print("è¯·å…ˆè®­ç»ƒæ¨¡å‹æˆ–æ£€æŸ¥æ¨¡å‹è·¯å¾„ï¼")
        return
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    print(f"ğŸ“‚ åŠ è½½æ¨¡å‹: {model_path}")
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.eval()

    csv_files = sorted([f for f in os.listdir(DATA_DIR) if f.endswith('.csv')])
    
    try:
        if not os.path.exists(LABELS_FILE):
            print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {LABELS_FILE}")
            print("è¯·å…ˆè¿è¡Œ create_lable.py åˆ›å»ºæ ‡ç­¾æ–‡ä»¶")
            return
        
        labels_df = pd.read_csv(LABELS_FILE, header=None)
        all_labels = labels_df.values.astype(np.float32)
        print(f"âœ… æˆåŠŸåŠ è½½ labels.csvï¼Œå…± {len(all_labels)} ä¸ªæ ·æœ¬")
    except Exception as e:
        print(f"âŒ æ— æ³•åŠ è½½ labels.csv: {e}")
        print(f"æ–‡ä»¶è·¯å¾„: {LABELS_FILE}")
        print("è¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼æ˜¯å¦æ­£ç¡®ï¼Œæˆ–é‡æ–°è¿è¡Œ create_lable.py")
        return

    # åœ°æ ‡ç‚¹åç§°ï¼ˆç”¨äºæ˜¾ç¤ºï¼‰
    landmark_names = ['Glabella', 'Nasion', 'Rhinion', 'Nasal Tip', 'Subnasale', 
                      'Alare (R)', 'Alare (L)', 'Zygion (R)', 'Zygion (L)']
    
    # å­˜å‚¨æ‰€æœ‰æ ·æœ¬çš„è¯¯å·®ç»Ÿè®¡
    all_rmse = []
    all_mae = []
    all_point_errors = []  # æ¯ä¸ªç‚¹çš„è¯¯å·®
    all_axis_errors = {'x': [], 'y': [], 'z': []}
    
    # é€‰æ‹©è¦è¯„ä¼°çš„æ ·æœ¬
    if evaluate_all:
        sample_indices = list(range(len(csv_files)))
        print(f"ğŸ“Š è¯„ä¼°æ‰€æœ‰ {len(sample_indices)} ä¸ªæ ·æœ¬...")
    else:
        sample_indices = [1, 11, 51]
        print(f"ğŸ“Š è¯„ä¼°æ ·æœ¬: {sample_indices}")
    
    # æ£€æŸ¥æ ·æœ¬
    for idx in sample_indices:
        filename = csv_files[idx]
        print(f"\n{'='*60}")
        print(f"--- Sample {idx}: {filename} ---")
        print(f"{'='*60}")
        
        try:
            # 1. åŸå§‹æ•°æ®
            df = pd.read_csv(os.path.join(DATA_DIR, filename))
            raw_points = df[['x', 'y', 'z']].values[:9].astype(np.float32)
            
            # 2. é¢„å¤„ç†ï¼šè®¡ç®—è´¨å¿ƒå¹¶å»ä¸­å¿ƒåŒ–
            centroid = np.mean(raw_points, axis=0)
            centered_points = raw_points - centroid
            
            # 3. é¢„æµ‹
            input_tensor = torch.from_numpy(centered_points).unsqueeze(0).transpose(2, 1).to(device)
            with torch.no_grad():
                pred_centered, _ = model(input_tensor)
            
            # 4. åå¤„ç†ï¼šè¿˜åŸä½ç½®
            pred_centered_np = pred_centered.numpy().reshape(9, 3)
            pred_final = pred_centered_np + centroid
            
            # 5. è®¡ç®—è¯¦ç»†è¯¯å·®
            gt_np = all_labels[idx].reshape(9, 3)
            
            # æ•´ä½“è¯¯å·®
            rmse = np.sqrt(np.mean((pred_final - gt_np) ** 2))
            mae = np.mean(np.abs(pred_final - gt_np))  # å¹³å‡ç»å¯¹è¯¯å·®
            all_rmse.append(rmse)
            all_mae.append(mae)
            
            # æ¯ä¸ªç‚¹çš„3Dæ¬§æ°è·ç¦»è¯¯å·®
            point_errors = np.sqrt(np.sum((pred_final - gt_np) ** 2, axis=1))
            all_point_errors.append(point_errors)
            
            # æ¯ä¸ªåæ ‡è½´çš„è¯¯å·®
            axis_errors = np.abs(pred_final - gt_np)  # (9, 3)
            x_error = np.mean(axis_errors[:, 0])
            y_error = np.mean(axis_errors[:, 1])
            z_error = np.mean(axis_errors[:, 2])
            all_axis_errors['x'].append(x_error)
            all_axis_errors['y'].append(y_error)
            all_axis_errors['z'].append(z_error)
            
            print(f"\nâœ… æ•´ä½“è¯¯å·® (å•ä½: æ¯«ç±³ mm):")
            print(f"   RMSE (å‡æ–¹æ ¹è¯¯å·®): {rmse:.4f} mm")
            print(f"   MAE (å¹³å‡ç»å¯¹è¯¯å·®): {mae:.4f} mm")
            print(f"\n   å„åæ ‡è½´å¹³å‡è¯¯å·® (å•ä½: æ¯«ç±³ mm):")
            print(f"   Xè½´: {x_error:.4f} mm")
            print(f"   Yè½´: {y_error:.4f} mm")
            print(f"   Zè½´: {z_error:.4f} mm")
            print(f"\n   æ¯ä¸ªåœ°æ ‡ç‚¹çš„3Dè·ç¦»è¯¯å·® (å•ä½: æ¯«ç±³ mm):")
            for i in range(9):
                print(f"   {landmark_names[i]:15s}: {point_errors[i]:.4f} mm")
            
            # æ˜¾ç¤ºæ¯ä¸ªç‚¹çš„è¯¦ç»†åæ ‡è¯¯å·®
            print(f"\n   æ¯ä¸ªç‚¹çš„è¯¦ç»†åæ ‡è¯¯å·® (å•ä½: æ¯«ç±³ mm, æ ¼å¼: X, Y, Z):")
            for i in range(9):
                x_err, y_err, z_err = axis_errors[i]
                print(f"   {landmark_names[i]:15s}: X={x_err:.4f} mm, Y={y_err:.4f} mm, Z={z_err:.4f} mm")

            # 6. ç»˜å›¾ (åªå¯¹éƒ¨åˆ†æ ·æœ¬ç”Ÿæˆå¯è§†åŒ–ï¼Œæˆ–è¯„ä¼°æ‰€æœ‰æ—¶åªç”Ÿæˆå‰å‡ ä¸ª)
            if not evaluate_all or idx < 5:  # åªç”Ÿæˆå‰5ä¸ªæ ·æœ¬çš„å¯è§†åŒ–
                fig = plt.figure(figsize=(8, 6))
                ax = fig.add_subplot(111, projection='3d')
                
                # Ground Truth (Green)
                ax.scatter(gt_np[:,0], gt_np[:,1], gt_np[:,2], c='g', s=50, label='Ground Truth')
                
                # Prediction (Red)
                ax.scatter(pred_final[:,0], pred_final[:,1], pred_final[:,2], c='r', marker='^', s=50, label='Prediction')
                
                for i in range(9):
                    ax.plot([gt_np[i,0], pred_final[i,0]], [gt_np[i,1], pred_final[i,1]], [gt_np[i,2], pred_final[i,2]], 'gray', linestyle='--')
                
                ax.set_title(f'Sample {idx} - RMSE: {rmse:.2f}')
                ax.set_xlabel('X')
                ax.set_ylabel('Y')
                ax.set_zlabel('Z')
                plt.legend()
                
            # ä¿å­˜å›¾ç‰‡è€Œä¸æ˜¯æ˜¾ç¤º
            # ä½¿ç”¨è‹±æ–‡æ–‡ä»¶åé¿å…ç¼–ç é—®é¢˜ï¼Œæ–‡ä»¶åæ ¼å¼ï¼šsample_ç´¢å¼•_ç»“æœ.png
            output_path = os.path.join(OUTPUT_DIR, f'sample_{idx:03d}_result.png')
            plt.savefig(output_path, dpi=150, bbox_inches='tight')
            plt.close()  # å…³é—­å›¾å½¢ä»¥é‡Šæ”¾å†…å­˜
            print(f"   å›¾ç‰‡å·²ä¿å­˜åˆ°: {output_path}")
            print(f"   æ–‡ä»¶å: sample_{idx:03d}_result.png (æ ·æœ¬ {idx} çš„é¢„æµ‹ç»“æœ)")
            
        except Exception as e:
            print(f"Skipping sample {idx}: {e}")
    
    # è®¡ç®—æ‰€æœ‰æ ·æœ¬çš„ç»Ÿè®¡ä¿¡æ¯
    if all_rmse:
        print(f"\n{'='*60}")
        print("ğŸ“Š æ‰€æœ‰æµ‹è¯•æ ·æœ¬çš„ç»Ÿè®¡ä¿¡æ¯")
        print(f"{'='*60}")
        print(f"\næ•´ä½“è¯¯å·®ç»Ÿè®¡ (å•ä½: æ¯«ç±³ mm):")
        print(f"   RMSE - å¹³å‡å€¼: {np.mean(all_rmse):.4f} mm, æ ‡å‡†å·®: {np.std(all_rmse):.4f} mm")
        print(f"   RMSE - æœ€å°å€¼: {np.min(all_rmse):.4f} mm, æœ€å¤§å€¼: {np.max(all_rmse):.4f} mm")
        print(f"   MAE - å¹³å‡å€¼: {np.mean(all_mae):.4f} mm, æ ‡å‡†å·®: {np.std(all_mae):.4f} mm")
        print(f"   MAE - æœ€å°å€¼: {np.min(all_mae):.4f} mm, æœ€å¤§å€¼: {np.max(all_mae):.4f} mm")
        
        print(f"\nå„åæ ‡è½´è¯¯å·®ç»Ÿè®¡ (å•ä½: æ¯«ç±³ mm):")
        for axis in ['x', 'y', 'z']:
            errors = all_axis_errors[axis]
            print(f"   {axis.upper()}è½´ - å¹³å‡å€¼: {np.mean(errors):.4f} mm, æ ‡å‡†å·®: {np.std(errors):.4f} mm")
            print(f"   {axis.upper()}è½´ - æœ€å°å€¼: {np.min(errors):.4f} mm, æœ€å¤§å€¼: {np.max(errors):.4f} mm")
        
        # è®¡ç®—æ¯ä¸ªåœ°æ ‡ç‚¹çš„å¹³å‡è¯¯å·®
        print(f"\næ¯ä¸ªåœ°æ ‡ç‚¹çš„å¹³å‡3Dè·ç¦»è¯¯å·® (å•ä½: æ¯«ç±³ mm):")
        point_errors_array = np.array(all_point_errors)  # (n_samples, 9)
        for i in range(9):
            avg_error = np.mean(point_errors_array[:, i])
            std_error = np.std(point_errors_array[:, i])
            print(f"   {landmark_names[i]:15s}: {avg_error:.4f} Â± {std_error:.4f} mm")
        
        # ä¿å­˜è¯¦ç»†è¯¯å·®æŠ¥å‘Šåˆ°CSV
        report_path = os.path.join(OUTPUT_DIR, 'error_report.csv')
        report_data = {
            'Sample': sample_indices[:len(all_rmse)],
            'RMSE': all_rmse,
            'MAE': all_mae,
            'X_Error': all_axis_errors['x'],
            'Y_Error': all_axis_errors['y'],
            'Z_Error': all_axis_errors['z']
        }
        # æ·»åŠ æ¯ä¸ªç‚¹çš„è¯¯å·®
        for i in range(9):
            report_data[f'{landmark_names[i]}_Error'] = [all_point_errors[j][i] for j in range(len(all_point_errors))]
        
        report_df = pd.DataFrame(report_data)
        report_df.to_csv(report_path, index=False)
        print(f"\nâœ… è¯¦ç»†è¯¯å·®æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")
        print(f"   æŠ¥å‘ŠåŒ…å« {len(report_df)} ä¸ªæ ·æœ¬çš„è¯¦ç»†è¯¯å·®ä¿¡æ¯")

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description='è¯„ä¼° PointNet å›å½’æ¨¡å‹')
    parser.add_argument('--all', action='store_true', help='è¯„ä¼°æ‰€æœ‰æ ·æœ¬ï¼ˆé»˜è®¤åªè¯„ä¼°æ ·æœ¬1, 11, 51ï¼‰')
    parser.add_argument('--model', type=str, default=None, 
                       help='æ¨¡å‹æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤: pointnet_regression_model.pthï¼‰')
    args = parser.parse_args()
    
    visualize_result(evaluate_all=args.all, model_path=args.model)