"""
PointNet å›å½’è®­ç»ƒè„šæœ¬ï¼ˆKæŠ˜äº¤å‰éªŒè¯ç‰ˆæœ¬ï¼‰
ä½¿ç”¨å®Œæ•´ç‚¹äº‘ä½œä¸ºè¾“å…¥ï¼Œé¢„æµ‹9ä¸ªåœ°æ ‡ç‚¹åæ ‡
é‡‡ç”¨KæŠ˜äº¤å‰éªŒè¯æä¾›æ›´å¯é çš„æ¨¡å‹è¯„ä¼°
"""
import os
import sys
import io
# è®¾ç½®UTF-8ç¼–ç ä»¥æ”¯æŒä¸­æ–‡è¾“å‡º
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')

# æ·»åŠ å½“å‰ç›®å½•åˆ° Python è·¯å¾„
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
if BASE_DIR not in sys.path:
    sys.path.insert(0, BASE_DIR)

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import TensorDataset, DataLoader, random_split
from sklearn.model_selection import KFold
from tqdm import tqdm
import json
from pointnet_utils import PointNetEncoder, feature_transform_reguliarzer

# =========================================================
# é…ç½®
# =========================================================
EXPORT_ROOT = os.path.join(BASE_DIR, 'data', 'pointcloud')
LABELS_FILE = os.path.join(BASE_DIR, 'labels.csv')
PROJECTS_LIST_FILE = os.path.join(BASE_DIR, 'valid_projects.txt')

NUM_TARGET_POINTS = 9
OUTPUT_DIM = NUM_TARGET_POINTS * 3  # 27ç»´

# GPU é…ç½®
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}")
if torch.cuda.is_available():
    print(f"   GPUåç§°: {torch.cuda.get_device_name(0)}")
    print(f"   GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")

# =========================================================
# è®­ç»ƒå‚æ•°é…ç½®
# =========================================================
BATCH_SIZE = 8               # ç‚¹äº‘è¾ƒå¤§ï¼ˆå¹³å‡19345ç‚¹ï¼‰ï¼Œä½¿ç”¨è¾ƒå°æ‰¹æ¬¡ä»¥é¿å…å†…å­˜ä¸è¶³
NUM_EPOCHS = 500
LEARNING_RATE = 0.001
LR_DECAY_STEP = 150
LR_DECAY_GAMMA = 0.5
DROPOUT_RATE = 0.3
FEATURE_TRANSFORM_WEIGHT = 0.001

# KæŠ˜äº¤å‰éªŒè¯é…ç½®
K_FOLDS = 5                  # 5æŠ˜äº¤å‰éªŒè¯
RANDOM_SEED = 42            # éšæœºç§å­ï¼Œç¡®ä¿å¯é‡å¤

# æµ‹è¯•é›†åˆ’åˆ†é…ç½®
TEST_RATIO = 0.1            # 10%ä½œä¸ºæµ‹è¯•é›†ï¼ˆç‹¬ç«‹è¯„ä¼°ç”¨ï¼‰

# æ¨¡å‹ä¿å­˜é…ç½®
MODEL_NAME_PREFIX = 'pointnet_regression_model_kfold'
BEST_MODEL_NAME = 'pointnet_regression_model_kfold_best.pth'

# =========================================================
# æ¨¡å‹å®šä¹‰
# =========================================================
class PointNetRegressor(nn.Module):
    def __init__(self, output_dim=27, dropout_rate=0.3):
        super(PointNetRegressor, self).__init__()
        self.feat = PointNetEncoder(global_feat=True, feature_transform=True, channel=3)
        self.fc1 = nn.Linear(1024, 512)
        self.fc2 = nn.Linear(512, 256)
        self.fc3 = nn.Linear(256, output_dim)
        self.bn1 = nn.BatchNorm1d(512)
        self.bn2 = nn.BatchNorm1d(256)
        self.dropout = nn.Dropout(p=dropout_rate)
        self.relu = nn.ReLU()

    def forward(self, x):
        x, trans, trans_feat = self.feat(x)
        x = self.relu(self.bn1(self.fc1(x)))
        x = self.relu(self.bn2(self.dropout(self.fc2(x))))
        x = self.fc3(x)
        return x, trans_feat

# =========================================================
# æ•°æ®åŠ è½½ï¼ˆå®Œæ•´ç‚¹äº‘ç‰ˆæœ¬ï¼‰
# =========================================================
def load_data():
    """
    åŠ è½½å®Œæ•´ç‚¹äº‘å’Œæ ‡ç­¾
    è¿”å›: X (torch.Tensor), Y (torch.Tensor)
    """
    print("--- æ­£åœ¨åŠ è½½å®Œæ•´ç‚¹äº‘å’Œæ ‡ç­¾ ---")
    
    # è¯»å–é¡¹ç›®åˆ—è¡¨
    if not os.path.exists(PROJECTS_LIST_FILE):
        raise FileNotFoundError(f"âŒ é¡¹ç›®åˆ—è¡¨æ–‡ä»¶ä¸å­˜åœ¨: {PROJECTS_LIST_FILE}\nè¯·å…ˆè¿è¡Œ create_labels_from_npy.py")
    
    with open(PROJECTS_LIST_FILE, 'r', encoding='utf-8') as f:
        project_names = [line.strip() for line in f if line.strip()]
    
    # è¯»å–æ ‡ç­¾æ–‡ä»¶
    if not os.path.exists(LABELS_FILE):
        raise FileNotFoundError(f"âŒ æ ‡ç­¾æ–‡ä»¶ä¸å­˜åœ¨: {LABELS_FILE}\nè¯·å…ˆè¿è¡Œ create_labels_from_npy.py")
    
    labels_df = pd.read_csv(LABELS_FILE, header=None)
    all_labels_np = labels_df.values.astype(np.float32)
    
    if len(project_names) != len(all_labels_np):
        print(f"âš ï¸  è­¦å‘Š: é¡¹ç›®æ•°é‡ ({len(project_names)}) ä¸æ ‡ç­¾æ•°é‡ ({len(all_labels_np)}) ä¸åŒ¹é…")
        min_len = min(len(project_names), len(all_labels_np))
        project_names = project_names[:min_len]
        all_labels_np = all_labels_np[:min_len]
    
    valid_features = []
    valid_labels = []
    point_counts = []
    
    print(f"åŠ è½½ {len(project_names)} ä¸ªæ ·æœ¬çš„ç‚¹äº‘...")
    
    for i, project_name in enumerate(tqdm(project_names, desc="åŠ è½½ç‚¹äº‘")):
        project_dir = os.path.join(EXPORT_ROOT, project_name)
        pointcloud_file = os.path.join(project_dir, "pointcloud_full.npy")
        
        if not os.path.exists(pointcloud_file):
            print(f"âš ï¸  è·³è¿‡ {project_name}: æœªæ‰¾åˆ° pointcloud_full.npy")
            continue
        
        try:
            # åŠ è½½å®Œæ•´ç‚¹äº‘
            pointcloud = np.load(pointcloud_file).astype(np.float32)  # shape: (N, 3)
            
            if len(pointcloud) == 0:
                print(f"âš ï¸  è·³è¿‡ {project_name}: ç‚¹äº‘ä¸ºç©º")
                continue
            
            # å…ˆè·å–æ ‡ç­¾
            current_label = all_labels_np[i].reshape(NUM_TARGET_POINTS, 3)
            
            # ä½¿ç”¨åœ°æ ‡ç‚¹è´¨å¿ƒä½œä¸ºå‚è€ƒç‚¹ï¼ˆè€Œä¸æ˜¯ç‚¹äº‘è´¨å¿ƒï¼‰
            # è¿™æ ·å¯ä»¥ç¡®ä¿ç‚¹äº‘å’Œåœ°æ ‡ç‚¹ä½¿ç”¨ç›¸åŒçš„å‚è€ƒåæ ‡ç³»
            label_centroid = np.mean(current_label, axis=0)
            
            # ç‚¹äº‘å’Œæ ‡ç­¾éƒ½ç›¸å¯¹äºåœ°æ ‡ç‚¹è´¨å¿ƒ
            centered_pointcloud = pointcloud - label_centroid
            centered_label = current_label - label_centroid
            
            # å½’ä¸€åŒ–ï¼šä½¿ç”¨ç‚¹äº‘çš„æ ‡å‡†å·®è¿›è¡Œç¼©æ”¾
            # è¿™æ ·å¯ä»¥ç»Ÿä¸€ä¸åŒæ ·æœ¬çš„å°ºåº¦
            scale = np.std(centered_pointcloud)
            if scale > 1e-6:  # é¿å…é™¤é›¶
                centered_pointcloud = centered_pointcloud / scale
                centered_label = centered_label / scale
            
            # è½¬ç½®ä¸º (3, N) æ ¼å¼ï¼Œé€‚é… PointNet
            centered_pointcloud_T = centered_pointcloud.T  # (3, N)
            
            valid_features.append(centered_pointcloud_T)
            valid_labels.append(centered_label.flatten())
            point_counts.append(len(pointcloud))
            
        except Exception as e:
            print(f"âŒ å¤„ç† {project_name} æ—¶å‡ºé”™: {e}")
            continue
    
    if not valid_features:
        raise RuntimeError("âŒ æœªèƒ½åŠ è½½ä»»ä½•æœ‰æ•ˆæ•°æ®ï¼")
    
    print(f"\nâœ… æˆåŠŸåŠ è½½ {len(valid_features)} ä¸ªæ ·æœ¬")
    print(f"   ç‚¹äº‘æ•°é‡ç»Ÿè®¡:")
    print(f"     æœ€å°: {min(point_counts)} ä¸ªç‚¹")
    print(f"     æœ€å¤§: {max(point_counts)} ä¸ªç‚¹")
    print(f"     å¹³å‡: {np.mean(point_counts):.0f} ä¸ªç‚¹")
    
    # ç»Ÿä¸€é‡‡æ ·åˆ°å›ºå®šç‚¹æ•°
    MAX_POINTS = 8192
    print(f"\nç»Ÿä¸€é‡‡æ ·åˆ° {MAX_POINTS} ä¸ªç‚¹...")
    
    processed_features = []
    for feat in valid_features:
        num_points = feat.shape[1]
        if num_points >= MAX_POINTS:
            # éšæœºé‡‡æ ·
            indices = np.random.choice(num_points, MAX_POINTS, replace=False)
            sampled_feat = feat[:, indices]
        else:
            # é‡å¤é‡‡æ ·ï¼ˆæœ‰æ”¾å›ï¼‰
            indices = np.random.choice(num_points, MAX_POINTS, replace=True)
            sampled_feat = feat[:, indices]
        processed_features.append(sampled_feat)
    
    # è½¬æ¢ä¸ºnumpyæ•°ç»„å¹¶è½¬ç½®ä¸º (N, 3, MAX_POINTS)
    X_np = np.array(processed_features, dtype=np.float32)  # (N, 3, MAX_POINTS)
    Y_np = np.array(valid_labels, dtype=np.float32)  # (N, 27)
    
    print(f"   æœ€ç»ˆæ•°æ®å½¢çŠ¶: X={X_np.shape}, Y={Y_np.shape}")
    
    return torch.from_numpy(X_np), torch.from_numpy(Y_np)

# =========================================================
# å•æŠ˜è®­ç»ƒå‡½æ•°
# =========================================================
def train_fold(X, Y, train_indices, val_indices, fold_num):
    """
    è®­ç»ƒå•ä¸ªæŠ˜
    
    å‚æ•°:
        X: ç‰¹å¾æ•°æ®
        Y: æ ‡ç­¾æ•°æ®
        train_indices: è®­ç»ƒé›†ç´¢å¼•
        val_indices: éªŒè¯é›†ç´¢å¼•
        fold_num: æŠ˜æ•°ï¼ˆ1-Kï¼‰
    
    è¿”å›:
        best_val_loss: æœ€ä½³éªŒè¯æŸå¤±
        training_history: è®­ç»ƒå†å²
    """
    # åˆ›å»ºæ•°æ®é›†
    train_dataset = TensorDataset(X[train_indices], Y[train_indices])
    val_dataset = TensorDataset(X[val_indices], Y[val_indices])
    
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)
    
    print(f"\n{'='*60}")
    print(f"ğŸ“Š æŠ˜ {fold_num}/{K_FOLDS}")
    print(f"{'='*60}")
    print(f"è®­ç»ƒé›†: {len(train_indices)} ä¸ªæ ·æœ¬")
    print(f"éªŒè¯é›†: {len(val_indices)} ä¸ªæ ·æœ¬")
    
    # åˆ›å»ºæ¨¡å‹
    model = PointNetRegressor(output_dim=OUTPUT_DIM, dropout_rate=DROPOUT_RATE).to(device)
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = torch.optim.lr_scheduler.StepLR(
        optimizer, step_size=LR_DECAY_STEP, gamma=LR_DECAY_GAMMA
    )
    
    best_val_loss = float('inf')
    training_history = {'train_loss': [], 'val_loss': [], 'epoch': []}
    
    for epoch in range(NUM_EPOCHS):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        total_train_loss = 0
        train_count = 0
        
        for data, target in train_loader:
            data, target = data.to(device), target.to(device)
            optimizer.zero_grad()
            pred, trans_feat = model(data)
            loss = criterion(pred, target)
            if trans_feat is not None:
                loss += feature_transform_reguliarzer(trans_feat) * FEATURE_TRANSFORM_WEIGHT
            loss.backward()
            optimizer.step()
            total_train_loss += loss.item()
            train_count += 1
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        total_val_loss = 0
        val_count = 0
        
        with torch.no_grad():
            for data, target in val_loader:
                data, target = data.to(device), target.to(device)
                pred, trans_feat = model(data)
                loss = criterion(pred, target)
                if trans_feat is not None:
                    loss += feature_transform_reguliarzer(trans_feat) * FEATURE_TRANSFORM_WEIGHT
                total_val_loss += loss.item()
                val_count += 1
        
        avg_train_loss = total_train_loss / train_count
        avg_val_loss = total_val_loss / val_count if val_count > 0 else 0
        
        training_history['train_loss'].append(avg_train_loss)
        training_history['val_loss'].append(avg_val_loss)
        training_history['epoch'].append(epoch + 1)
        
        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step()
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            fold_model_path = os.path.join(BASE_DIR, f'{MODEL_NAME_PREFIX}_fold{fold_num}_best.pth')
            torch.save(model.state_dict(), fold_model_path)
        
        # æ‰“å°è¿›åº¦
        if (epoch+1) % 50 == 0 or epoch == 0:
            print(f"  Epoch {epoch+1:4d}/{NUM_EPOCHS} | "
                  f"Train Loss: {avg_train_loss:.6f} | "
                  f"Val Loss: {avg_val_loss:.6f} | "
                  f"Best Val: {best_val_loss:.6f}")
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    fold_final_path = os.path.join(BASE_DIR, f'{MODEL_NAME_PREFIX}_fold{fold_num}_final.pth')
    torch.save(model.state_dict(), fold_final_path)
    
    print(f"\nâœ… æŠ˜ {fold_num} è®­ç»ƒå®Œæˆ")
    print(f"   æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}")
    print(f"   æœ€ä½³æ¨¡å‹: {fold_model_path}")
    print(f"   æœ€ç»ˆæ¨¡å‹: {fold_final_path}")
    
    return best_val_loss, training_history

# =========================================================
# KæŠ˜äº¤å‰éªŒè¯ä¸»å‡½æ•°
# =========================================================
def train_kfold():
    """
    KæŠ˜äº¤å‰éªŒè¯è®­ç»ƒï¼ˆåŒ…å«ç‹¬ç«‹æµ‹è¯•é›†ï¼‰
    """
    # åŠ è½½æ•°æ®
    X, Y = load_data()
    if X is None:
        return
    
    print(f"\n{'='*60}")
    print(f"ğŸ”„ KæŠ˜äº¤å‰éªŒè¯è®­ç»ƒ (K={K_FOLDS}) + ç‹¬ç«‹æµ‹è¯•é›†")
    print(f"{'='*60}")
    print(f"æ€»æ ·æœ¬æ•°: {len(X)}")
    
    # =========================================================
    # ç¬¬ä¸€æ­¥ï¼šåˆ’åˆ†æµ‹è¯•é›†ï¼ˆ10%ï¼‰
    # =========================================================
    from torch.utils.data import random_split
    
    dataset = TensorDataset(X, Y)
    test_size = int(TEST_RATIO * len(dataset))
    train_val_size = len(dataset) - test_size
    
    # åˆ’åˆ†æµ‹è¯•é›†å’Œè®­ç»ƒ+éªŒè¯é›†
    train_val_dataset, test_dataset = random_split(
        dataset, [train_val_size, test_size], 
        generator=torch.Generator().manual_seed(RANDOM_SEED)
    )
    
    # æå–è®­ç»ƒ+éªŒè¯é›†çš„æ•°æ®
    train_val_indices = train_val_dataset.indices
    test_indices = test_dataset.indices
    
    train_val_X = X[train_val_indices]
    train_val_Y = Y[train_val_indices]
    test_X = X[test_indices]
    test_Y = Y[test_indices]
    
    print(f"\nğŸ“Š æ•°æ®åˆ’åˆ†:")
    print(f"   æµ‹è¯•é›†: {len(test_indices)} ä¸ªæ ·æœ¬ ({TEST_RATIO*100:.0f}%)")
    print(f"   è®­ç»ƒ+éªŒè¯é›†: {len(train_val_indices)} ä¸ªæ ·æœ¬ ({(1-TEST_RATIO)*100:.0f}%)")
    print(f"   æ¯æŠ˜éªŒè¯é›†å¤§å°: çº¦ {len(train_val_indices) // K_FOLDS} ä¸ªæ ·æœ¬")
    print(f"   æ¯æŠ˜è®­ç»ƒé›†å¤§å°: çº¦ {len(train_val_indices) - len(train_val_indices) // K_FOLDS} ä¸ªæ ·æœ¬")
    
    # =========================================================
    # ç¬¬äºŒæ­¥ï¼šåœ¨è®­ç»ƒ+éªŒè¯é›†ä¸ŠåšKæŠ˜äº¤å‰éªŒè¯
    # =========================================================
    # åˆ›å»ºKæŠ˜åˆ’åˆ†ï¼ˆåœ¨90%çš„æ•°æ®ä¸Šï¼‰
    kfold = KFold(n_splits=K_FOLDS, shuffle=True, random_state=RANDOM_SEED)
    
    # å­˜å‚¨æ‰€æœ‰æŠ˜çš„ç»“æœ
    fold_results = []
    all_training_histories = []
    
    # è®­ç»ƒæ¯ä¸€æŠ˜ï¼ˆåœ¨è®­ç»ƒ+éªŒè¯é›†ä¸Šï¼‰
    # KFoldéœ€è¦numpyæ•°ç»„ï¼Œä½¿ç”¨ç´¢å¼•æ•°ç»„
    train_val_indices_array = np.array(train_val_indices)
    
    for fold_num, (fold_train_idx, fold_val_idx) in enumerate(kfold.split(train_val_indices_array), 1):
        # å°†æŠ˜å†…çš„ç´¢å¼•æ˜ å°„å›åŸå§‹ç´¢å¼•
        train_indices = train_val_indices_array[fold_train_idx].tolist()
        val_indices = train_val_indices_array[fold_val_idx].tolist()
        
        best_val_loss, training_history = train_fold(
            X, Y, train_indices, val_indices, fold_num
        )
        fold_results.append({
            'fold': fold_num,
            'best_val_loss': best_val_loss,
            'train_size': len(train_indices),
            'val_size': len(val_indices)
        })
        all_training_histories.append(training_history)
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    val_losses = [r['best_val_loss'] for r in fold_results]
    mean_val_loss = np.mean(val_losses)
    std_val_loss = np.std(val_losses)
    min_val_loss = np.min(val_losses)
    best_fold = np.argmin(val_losses) + 1
    
    # æ‰“å°æ€»ç»“
    print(f"\n{'='*60}")
    print(f"ğŸ“Š KæŠ˜äº¤å‰éªŒè¯ç»“æœæ€»ç»“")
    print(f"{'='*60}")
    for result in fold_results:
        print(f"æŠ˜ {result['fold']}: æœ€ä½³éªŒè¯æŸå¤± = {result['best_val_loss']:.6f}")
    print(f"\nç»Ÿè®¡ä¿¡æ¯:")
    print(f"  å¹³å‡éªŒè¯æŸå¤±: {mean_val_loss:.6f} Â± {std_val_loss:.6f}")
    print(f"  æœ€å°éªŒè¯æŸå¤±: {min_val_loss:.6f} (æŠ˜ {best_fold})")
    print(f"  æ ‡å‡†å·®: {std_val_loss:.6f}")
    
    # é€‰æ‹©æœ€ä½³æŠ˜çš„æ¨¡å‹ä½œä¸ºæœ€ç»ˆæ¨¡å‹
    best_model_source = os.path.join(BASE_DIR, f'{MODEL_NAME_PREFIX}_fold{best_fold}_best.pth')
    best_model_dest = os.path.join(BASE_DIR, BEST_MODEL_NAME)
    
    import shutil
    shutil.copy(best_model_source, best_model_dest)
    
    print(f"\nâœ… æœ€ä½³æ¨¡å‹å·²å¤åˆ¶:")
    print(f"   æ¥æº: æŠ˜ {best_fold} çš„æœ€ä½³æ¨¡å‹")
    print(f"   ç›®æ ‡: {best_model_dest}")
    
    # ä¿å­˜æ‰€æœ‰æŠ˜çš„è®­ç»ƒå†å²
    kfold_history = {
        'k_folds': K_FOLDS,
        'fold_results': fold_results,
        'statistics': {
            'mean_val_loss': float(mean_val_loss),
            'std_val_loss': float(std_val_loss),
            'min_val_loss': float(min_val_loss),
            'best_fold': int(best_fold)
        },
        'training_histories': all_training_histories
    }
    
    history_path = os.path.join(BASE_DIR, 'training_history_kfold.json')
    with open(history_path, 'w') as f:
        json.dump(kfold_history, f, indent=2)
    
    print(f"   è®­ç»ƒå†å²: {history_path}")
    
    # =========================================================
    # ç¬¬ä¸‰æ­¥ï¼šç”¨æ‰€æœ‰è®­ç»ƒ+éªŒè¯æ•°æ®é‡æ–°è®­ç»ƒæœ€ç»ˆæ¨¡å‹ï¼ˆå¸¦éªŒè¯é›†å’Œæ—©åœï¼‰
    # =========================================================
    print(f"\n{'='*60}")
    print(f"ğŸ”„ ç”¨æ‰€æœ‰è®­ç»ƒ+éªŒè¯æ•°æ®é‡æ–°è®­ç»ƒæœ€ç»ˆæ¨¡å‹ï¼ˆå¸¦éªŒè¯é›†å’Œæ—©åœï¼‰")
    print(f"{'='*60}")
    print(f"æ€»æ•°æ®: {len(train_val_indices)} ä¸ªæ ·æœ¬ï¼ˆæ‰€æœ‰90%çš„æ•°æ®ï¼‰")
    print(f"ç›®çš„: å……åˆ†åˆ©ç”¨æ‰€æœ‰æ•°æ®ï¼ŒåŒæ—¶é¿å…è¿‡æ‹Ÿåˆ")
    
    # ä»90%çš„æ•°æ®ä¸­å†åˆ†å‡º10%ä½œä¸ºéªŒè¯é›†ï¼ˆç”¨äºæ—©åœï¼‰
    # è¿™æ ·æœ€ç»ˆè®­ç»ƒé›†æ˜¯80%ï¼ŒéªŒè¯é›†æ˜¯10%ï¼Œæµ‹è¯•é›†æ˜¯10%
    final_val_ratio = 0.1  # ä»90%ä¸­åˆ†å‡º10%ä½œä¸ºéªŒè¯é›†
    final_train_val_dataset = TensorDataset(train_val_X, train_val_Y)
    final_train_size = int((1 - final_val_ratio) * len(final_train_val_dataset))
    final_val_size = len(final_train_val_dataset) - final_train_size
    
    # åˆ’åˆ†æœ€ç»ˆè®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼ˆä½¿ç”¨ä¸åŒçš„éšæœºç§å­ï¼Œç¡®ä¿ä¸æµ‹è¯•é›†åˆ’åˆ†ä¸åŒï¼‰
    final_train_dataset, final_val_dataset = random_split(
        final_train_val_dataset, [final_train_size, final_val_size],
        generator=torch.Generator().manual_seed(RANDOM_SEED + 100)  # ä½¿ç”¨ä¸åŒçš„éšæœºç§å­
    )
    
    final_train_loader = DataLoader(
        final_train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True
    )
    final_val_loader = DataLoader(
        final_val_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=False
    )
    
    print(f"   æœ€ç»ˆè®­ç»ƒé›†: {len(final_train_dataset)} ä¸ªæ ·æœ¬ ({final_train_size/len(train_val_indices)*100:.1f}%)")
    print(f"   æœ€ç»ˆéªŒè¯é›†: {len(final_val_dataset)} ä¸ªæ ·æœ¬ ({final_val_size/len(train_val_indices)*100:.1f}%)")
    print(f"   æµ‹è¯•é›†: {len(test_indices)} ä¸ªæ ·æœ¬ (10%)")
    
    # åˆ›å»ºæ–°çš„æ¨¡å‹ï¼ˆä»å¤´è®­ç»ƒï¼‰
    final_model = PointNetRegressor(output_dim=OUTPUT_DIM, dropout_rate=DROPOUT_RATE).to(device)
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(final_model.parameters(), lr=LEARNING_RATE)
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆä½¿ç”¨ä¸KæŠ˜ç›¸åŒçš„é…ç½®ï¼‰
    scheduler = torch.optim.lr_scheduler.StepLR(
        optimizer, step_size=LR_DECAY_STEP, gamma=LR_DECAY_GAMMA
    )
    
    # æ—©åœé…ç½®
    PATIENCE = 50  # å¦‚æœéªŒè¯æŸå¤±åœ¨50è½®å†…æ²¡æœ‰æ”¹å–„ï¼Œåˆ™æ—©åœ
    best_val_loss = float('inf')
    patience_counter = 0
    best_epoch = 0
    
    print(f"\nå¼€å§‹è®­ç»ƒæœ€ç»ˆæ¨¡å‹ï¼ˆæœ€å¤š{NUM_EPOCHS}è½®ï¼Œæ—©åœè€å¿ƒ={PATIENCE}ï¼‰...")
    final_model.train()
    final_training_history = {'train_loss': [], 'val_loss': [], 'epoch': []}
    
    for epoch in range(NUM_EPOCHS):
        # è®­ç»ƒé˜¶æ®µ
        final_model.train()
        total_train_loss = 0
        train_count = 0
        
        for data, target in final_train_loader:
            data, target = data.to(device), target.to(device)
            optimizer.zero_grad()
            pred, trans_feat = final_model(data)
            loss = criterion(pred, target)
            if trans_feat is not None:
                loss += feature_transform_reguliarzer(trans_feat) * FEATURE_TRANSFORM_WEIGHT
            loss.backward()
            optimizer.step()
            total_train_loss += loss.item()
            train_count += 1
        
        avg_train_loss = total_train_loss / train_count
        
        # éªŒè¯é˜¶æ®µ
        final_model.eval()
        total_val_loss = 0
        val_count = 0
        
        with torch.no_grad():
            for data, target in final_val_loader:
                data, target = data.to(device), target.to(device)
                pred, trans_feat = final_model(data)
                loss = criterion(pred, target)
                if trans_feat is not None:
                    loss += feature_transform_reguliarzer(trans_feat) * FEATURE_TRANSFORM_WEIGHT
                total_val_loss += loss.item()
                val_count += 1
        
        avg_val_loss = total_val_loss / val_count if val_count > 0 else float('inf')
        
        final_training_history['train_loss'].append(avg_train_loss)
        final_training_history['val_loss'].append(avg_val_loss)
        final_training_history['epoch'].append(epoch + 1)
        
        scheduler.step()
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ä½³æ¨¡å‹
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            best_epoch = epoch + 1
            patience_counter = 0
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            best_model_state = final_model.state_dict().copy()
        else:
            patience_counter += 1
        
        # æ‰“å°è¿›åº¦
        if (epoch+1) % 50 == 0 or epoch == 0:
            print(f"  Epoch {epoch+1:4d}/{NUM_EPOCHS} | Train Loss: {avg_train_loss:.6f} | Val Loss: {avg_val_loss:.6f} | Best Val: {best_val_loss:.6f} (Epoch {best_epoch})")
        
        # æ—©åœæ£€æŸ¥
        if patience_counter >= PATIENCE:
            print(f"\nâš ï¸  æ—©åœè§¦å‘ï¼éªŒè¯æŸå¤±åœ¨{PATIENCE}è½®å†…æ²¡æœ‰æ”¹å–„")
            print(f"   æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f} (Epoch {best_epoch})")
            break
    
    # åŠ è½½æœ€ä½³æ¨¡å‹
    final_model.load_state_dict(best_model_state)
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹ï¼ˆæœ€ä½³æ¨¡å‹ï¼‰
    final_model_path = os.path.join(BASE_DIR, BEST_MODEL_NAME)
    torch.save(final_model.state_dict(), final_model_path)
    
    print(f"\nâœ… æœ€ç»ˆæ¨¡å‹è®­ç»ƒå®Œæˆï¼")
    print(f"   æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f} (Epoch {best_epoch})")
    print(f"   æœ€ç»ˆè®­ç»ƒæŸå¤±: {final_training_history['train_loss'][best_epoch-1]:.6f}")
    print(f"   æ¨¡å‹å·²ä¿å­˜: {final_model_path}")
    print(f"   ä½¿ç”¨æ•°æ®: {len(final_train_dataset)} ä¸ªæ ·æœ¬è®­ç»ƒï¼Œ{len(final_val_dataset)} ä¸ªæ ·æœ¬éªŒè¯")
    
    # =========================================================
    # ç¬¬å››æ­¥ï¼šåœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æœ€ç»ˆæ¨¡å‹
    # =========================================================
    print(f"\n{'='*60}")
    print(f"ğŸ§ª åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æœ€ç»ˆæ¨¡å‹")
    print(f"{'='*60}")
    
    # ä½¿ç”¨æœ€ç»ˆè®­ç»ƒçš„æ¨¡å‹
    final_model.eval()
    
    # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
    test_dataset_final = TensorDataset(test_X, test_Y)
    test_loader = DataLoader(test_dataset_final, batch_size=BATCH_SIZE, shuffle=False)
    
    criterion = nn.MSELoss()
    total_test_loss = 0
    test_count = 0
    
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            pred, trans_feat = final_model(data)
            loss = criterion(pred, target)
            if trans_feat is not None:
                loss += feature_transform_reguliarzer(trans_feat) * FEATURE_TRANSFORM_WEIGHT
            total_test_loss += loss.item()
            test_count += 1
    
    avg_test_loss = total_test_loss / test_count if test_count > 0 else 0
    
    print(f"æµ‹è¯•é›†å¤§å°: {len(test_indices)} ä¸ªæ ·æœ¬")
    print(f"æµ‹è¯•é›†æŸå¤±: {avg_test_loss:.6f}")
    
    # æ›´æ–°è®­ç»ƒå†å²ï¼Œæ·»åŠ æœ€ç»ˆæ¨¡å‹å’Œæµ‹è¯•é›†ç»“æœ
    kfold_history['final_model'] = {
        'training_history': final_training_history,
        'train_size': len(final_train_dataset),
        'val_size': len(final_val_dataset),
        'best_epoch': best_epoch,
        'best_val_loss': float(best_val_loss),
        'final_train_loss': float(final_training_history['train_loss'][best_epoch-1]),
        'early_stopped': patience_counter >= PATIENCE
    }
    kfold_history['test_loss'] = float(avg_test_loss)
    kfold_history['test_size'] = len(test_indices)
    
    # é‡æ–°ä¿å­˜è®­ç»ƒå†å²ï¼ˆåŒ…å«æœ€ç»ˆæ¨¡å‹å’Œæµ‹è¯•é›†ç»“æœï¼‰
    with open(history_path, 'w') as f:
        json.dump(kfold_history, f, indent=2)
    
    print(f"\nğŸ‰ KæŠ˜äº¤å‰éªŒè¯ + æœ€ç»ˆæ¨¡å‹è®­ç»ƒå®Œæˆï¼")
    print(f"   KæŠ˜äº¤å‰éªŒè¯: è®­ç»ƒäº† {K_FOLDS} ä¸ªæ¨¡å‹ç”¨äºé€‰æ‹©æœ€ä½³é…ç½®")
    print(f"   å¹³å‡éªŒè¯æŸå¤±: {mean_val_loss:.6f} Â± {std_val_loss:.6f}")
    print(f"   æœ€ä½³æŠ˜: æŠ˜ {best_fold}ï¼ŒéªŒè¯æŸå¤±: {min_val_loss:.6f}")
    print(f"   æœ€ç»ˆæ¨¡å‹: ç”¨ {len(final_train_dataset)} ä¸ªæ ·æœ¬è®­ç»ƒï¼Œ{len(final_val_dataset)} ä¸ªæ ·æœ¬éªŒè¯")
    print(f"   æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f} (Epoch {best_epoch})")
    if patience_counter >= PATIENCE:
        print(f"   âš ï¸  æ—©åœè§¦å‘ï¼ˆè€å¿ƒ={PATIENCE}ï¼‰")
    print(f"   â­ æµ‹è¯•é›†æŸå¤±: {avg_test_loss:.6f} (ç‹¬ç«‹è¯„ä¼°ï¼Œæ— å)")
    print(f"\nğŸ“ æœ€ç»ˆæ¨¡å‹æ–‡ä»¶: {final_model_path}")

if __name__ == "__main__":
    train_kfold()
