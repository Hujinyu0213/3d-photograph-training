"""
è¯„ä¼°å¢å¼º+FPSæ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„æ€§èƒ½
å¹¶ä¸KæŠ˜æ¨¡å‹å¯¹æ¯”
"""
import os
import sys
import json
import numpy as np
import pandas as pd
import torch
import torch.nn as nn

# è·¯å¾„è®¾ç½®
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
ROOT_DIR = os.path.dirname(os.path.dirname(BASE_DIR))
UTILS_DIR = os.path.join(ROOT_DIR, "scripts", "utils")
for p in (ROOT_DIR, UTILS_DIR):
    if p not in sys.path:
        sys.path.insert(0, p)

import importlib.util
_pn_path = os.path.join(UTILS_DIR, "pointnet_utils.py")
spec = importlib.util.spec_from_file_location("pointnet_utils", _pn_path)
pointnet_utils = importlib.util.module_from_spec(spec)
spec.loader.exec_module(pointnet_utils)
PointNetEncoder = pointnet_utils.PointNetEncoder

EXPORT_ROOT = os.path.join(ROOT_DIR, 'data', 'pointcloud')
LABELS_FILE = os.path.join(ROOT_DIR, 'results', 'labels.csv')
PROJECTS_LIST_FILE = os.path.join(ROOT_DIR, 'results', 'valid_projects.txt')

NUM_TARGET_POINTS = 9
OUTPUT_DIM = NUM_TARGET_POINTS * 3
MAX_POINTS = 8192

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# æ¨¡å‹å®šä¹‰
class PointNetRegressor(nn.Module):
    def __init__(self, output_dim=27, dropout_rate=0.3):
        super().__init__()
        self.feat = PointNetEncoder(global_feat=True, feature_transform=True, channel=3)
        self.fc1 = nn.Linear(1024, 512)
        self.fc2 = nn.Linear(512, 256)
        self.fc3 = nn.Linear(256, output_dim)
        self.bn1 = nn.BatchNorm1d(512)
        self.bn2 = nn.BatchNorm1d(256)
        self.dropout = nn.Dropout(p=dropout_rate)
        self.relu = nn.ReLU()

    def forward(self, x):
        x, trans, trans_feat = self.feat(x)
        x = self.relu(self.bn1(self.fc1(x)))
        x = self.relu(self.bn2(self.dropout(self.fc2(x))))
        x = self.fc3(x)
        return x, trans_feat

# FPS
def farthest_point_sampling(points_np, n_samples):
    N = points_np.shape[0]
    if N <= n_samples:
        idx = np.random.choice(N, n_samples, replace=True)
        return points_np[idx]
    farthest_pts = np.zeros((n_samples,), dtype=np.int64)
    distances = np.full((N,), np.inf)
    farthest = np.random.randint(0, N)
    for i in range(n_samples):
        farthest_pts[i] = farthest
        centroid = points_np[farthest][None, :]
        dist = np.sum((points_np - centroid) ** 2, axis=1)
        distances = np.minimum(distances, dist)
        farthest = np.argmax(distances)
    return points_np[farthest_pts]

# åŠ è½½æ•°æ®
def load_test_data():
    with open(PROJECTS_LIST_FILE, 'r', encoding='utf-8') as f:
        project_names = [ln.strip() for ln in f if ln.strip()]
    labels_df = pd.read_csv(LABELS_FILE, header=None)
    labels_np = labels_df.values.astype(np.float32)
    
    # ä½¿ç”¨ä¸è®­ç»ƒç›¸åŒçš„åˆ’åˆ†ï¼ˆå20%ä½œä¸ºæµ‹è¯•é›†ï¼‰
    test_size = max(1, int(0.2 * len(project_names)))
    test_indices = list(range(len(project_names) - test_size, len(project_names)))
    
    feats = []
    labels = []
    names = []
    scales = []
    
    for idx in test_indices:
        name = project_names[idx]
        pc_path = os.path.join(EXPORT_ROOT, name, "pointcloud_full.npy")
        if not os.path.exists(pc_path):
            continue
        pc = np.load(pc_path).astype(np.float32)
        if pc.shape[0] == 0:
            continue
        
        label = labels_np[idx].reshape(NUM_TARGET_POINTS, 3)
        label_centroid = np.mean(label, axis=0)
        pc_centered = pc - label_centroid
        label_centered = label - label_centroid
        
        scale = np.std(pc_centered)
        if scale > 1e-6:
            pc_centered /= scale
            label_centered /= scale
        
        pc_sampled = farthest_point_sampling(pc_centered, MAX_POINTS)
        feats.append(pc_sampled.T)
        labels.append(label_centered.flatten())
        names.append(name)
        scales.append(scale if scale > 1e-6 else 1.0)
    
    X = torch.from_numpy(np.stack(feats, axis=0)).float()
    Y = torch.from_numpy(np.stack(labels, axis=0)).float()
    scales = np.array(scales, dtype=np.float32)
    return X, Y, names, scales

# è¯„ä¼°å‡½æ•°
def evaluate_model(model_path, model_name):
    print(f"\n{'='*60}")
    print(f"è¯„ä¼°æ¨¡å‹: {model_name}")
    print(f"æ¨¡å‹è·¯å¾„: {model_path}")
    print(f"{'='*60}")
    
    X, Y, names, scales = load_test_data()
    print(f"æµ‹è¯•é›†æ ·æœ¬æ•°: {len(X)}")
    
    model = PointNetRegressor(output_dim=OUTPUT_DIM, dropout_rate=0.3).to(device)
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.eval()
    
    predictions = []
    with torch.no_grad():
        for i in range(len(X)):
            data = X[i:i+1].to(device)
            pred, _ = model(data)
            predictions.append(pred.cpu().numpy()[0])
    
    predictions = np.array(predictions)          # normalized
    targets = Y.numpy()                          # normalized

    # åå½’ä¸€åŒ–åˆ°åŸå§‹å•ä½ï¼ˆå‡è®¾åŸå§‹å•ä½ä¸ºæ¯«ç±³ï¼‰
    scales_expanded = scales[:, None, None]      # (N,1,1)
    preds_mm = predictions.reshape(len(X), NUM_TARGET_POINTS, 3) * scales_expanded
    targets_mm = targets.reshape(len(X), NUM_TARGET_POINTS, 3) * scales_expanded

    # è®¡ç®—æŒ‡æ ‡ï¼ˆæ¯«ç±³ï¼‰
    errors_mm = preds_mm - targets_mm
    mse_mm = np.mean(errors_mm ** 2)
    rmse_mm = np.sqrt(mse_mm)
    mae_mm = np.mean(np.abs(errors_mm))

    # æ¯ä¸ªç‚¹çš„3Dè¯¯å·®ï¼ˆæ¯«ç±³ï¼‰
    point_errors = []
    for i in range(NUM_TARGET_POINTS):
        pred_pt = preds_mm[:, i, :]
        true_pt = targets_mm[:, i, :]
        dist_3d = np.sqrt(np.sum((pred_pt - true_pt)**2, axis=1))
        point_errors.append({
            'point_id': i+1,
            'rmse_3d_mm': float(np.sqrt(np.mean(dist_3d**2))),
            'mae_3d_mm': float(np.mean(dist_3d)),
            'max_mm': float(np.max(dist_3d)),
            'min_mm': float(np.min(dist_3d))
        })

    results = {
        'model_name': model_name,
        'test_samples': len(X),
        'overall': {
            'mse_mm': float(mse_mm),
            'rmse_mm': float(rmse_mm),
            'mae_mm': float(mae_mm)
        },
        'per_point': point_errors
    }
    
    return results

# ä¸»å‡½æ•°
def main():
    print("="*60)
    print("ğŸ§ª å¢å¼º+FPSæ¨¡å‹ vs KæŠ˜æ¨¡å‹æµ‹è¯•é›†å¯¹æ¯”è¯„ä¼°")
    print("="*60)
    
    # è¯„ä¼°æ–°æ¨¡å‹
    aug_fps_model = os.path.join(ROOT_DIR, "models", "pointnet_regression_model_full_aug_fps_best.pth")
    aug_results = evaluate_model(aug_fps_model, "æ•°æ®å¢å¼º+FPSæ¨¡å‹")
    
    # è¯„ä¼°KæŠ˜+å¢å¼º+FPSæ¨¡å‹
    kfold_aug_fps_model = os.path.join(ROOT_DIR, "models", "pointnet_regression_model_kfold_aug_fps_best.pth")
    if os.path.exists(kfold_aug_fps_model):
        kfold_results = evaluate_model(kfold_aug_fps_model, "KæŠ˜+å¢å¼º+FPSæ¨¡å‹")
    else:
        kfold_results = None
        print("\nâš ï¸  KæŠ˜+å¢å¼º+FPSæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡å¯¹æ¯”")
    
    # æ‰“å°å¯¹æ¯”ç»“æœ
    print(f"\n{'='*60}")
    print("ğŸ“Š æµ‹è¯•é›†æ€§èƒ½å¯¹æ¯” (æ¯«ç±³)")
    print(f"{'='*60}")
    
    print(f"\næ•´ä½“æŒ‡æ ‡å¯¹æ¯” (æ¯«ç±³):")
    print(f"{'æŒ‡æ ‡':<20} {'å¢å¼º+FPS':>15} {'KæŠ˜æ¨¡å‹':>15} {'æ”¹è¿›':>10}")
    print("-"*60)
    
    metrics = ['mse_mm', 'rmse_mm', 'mae_mm']
    metric_names = {'mse_mm': 'MSE(mm^2)', 'rmse_mm': 'RMSE(mm)', 'mae_mm': 'MAE(mm)'}
    
    for metric in metrics:
        aug_val = aug_results['overall'][metric]
        if kfold_results:
            kfold_val = kfold_results['overall'][metric]
            improvement = ((kfold_val - aug_val) / kfold_val) * 100
            print(f"{metric_names[metric]:<20} {aug_val:>15.6f} {kfold_val:>15.6f} {improvement:>9.2f}%")
        else:
            print(f"{metric_names[metric]:<20} {aug_val:>15.6f} {'N/A':>15} {'N/A':>10}")
    
    # æ¯ä¸ªåœ°æ ‡ç‚¹å¯¹æ¯”
    landmark_names = ['Glabella', 'Nasion', 'Rhinion', 'Nasal Tip', 'Subnasale',
                      'Alare (R)', 'Alare (L)', 'Zygion (R)', 'Zygion (L)']
    
    print(f"\næ¯ä¸ªåœ°æ ‡ç‚¹3Dè¯¯å·®å¯¹æ¯” (RMSE, mm):")
    print(f"{'åœ°æ ‡ç‚¹':<15} {'å¢å¼º+FPS':>12} {'KæŠ˜æ¨¡å‹':>12} {'æ”¹è¿›':>10}")
    print("-"*60)
    
    for i in range(NUM_TARGET_POINTS):
        aug_rmse = aug_results['per_point'][i]['rmse_3d_mm']
        name = landmark_names[i] if i < len(landmark_names) else f"Point {i+1}"
        if kfold_results:
            kfold_rmse = kfold_results['per_point'][i]['rmse_3d_mm']
            improvement = ((kfold_rmse - aug_rmse) / kfold_rmse) * 100
            print(f"{name:<15} {aug_rmse:>12.6f} {kfold_rmse:>12.6f} {improvement:>9.2f}%")
        else:
            print(f"{name:<15} {aug_rmse:>12.6f} {'N/A':>12} {'N/A':>10}")
    
    # ä¿å­˜ç»“æœ
    output = {
        'aug_fps_model': aug_results,
        'kfold_model': kfold_results
    }
    
    output_path = os.path.join(ROOT_DIR, "results", "test_comparison_aug_fps_vs_kfold_aug_fps.json")
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(output, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ“ è¯¦ç»†ç»“æœå·²ä¿å­˜: {output_path}")
    
    # ç»“è®º
    print(f"\n{'='*60}")
    print("ğŸ’¡ ç»“è®ºä¸å»ºè®®")
    print(f"{'='*60}")
    
    if kfold_results:
        overall_improvement = ((kfold_results['overall']['rmse_mm'] - aug_results['overall']['rmse_mm']) / 
                              kfold_results['overall']['rmse_mm']) * 100
        
        if overall_improvement > 10:
            print(f"âœ… å¢å¼º+FPSæ¨¡å‹æ˜¾è‘—ä¼˜äºKæŠ˜æ¨¡å‹ (RMSEæ”¹è¿› {overall_improvement:.2f}%)")
            print(f"   å»ºè®®: é‡‡ç”¨å¢å¼º+FPSæ¨¡å‹ä½œä¸ºæœ€ç»ˆæ¨¡å‹")
        elif overall_improvement > 0:
            print(f"âœ… å¢å¼º+FPSæ¨¡å‹ç•¥ä¼˜äºKæŠ˜æ¨¡å‹ (RMSEæ”¹è¿› {overall_improvement:.2f}%)")
            print(f"   å»ºè®®: å¯ä»¥ä½¿ç”¨å¢å¼º+FPSæ¨¡å‹ï¼Œæˆ–ç»“åˆä¸¤è€…ä¼˜ç‚¹")
        else:
            print(f"âš ï¸  KæŠ˜æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šè¡¨ç°æ›´å¥½")
            print(f"   å»ºè®®: æ£€æŸ¥å¢å¼ºå‚æ•°æ˜¯å¦è¿‡å¼ºï¼Œæˆ–å¢åŠ è®­ç»ƒæ ·æœ¬")
    
    print(f"\nä¸‹ä¸€æ­¥:")
    print(f"  1. å¦‚æœå¢å¼º+FPSæ•ˆæœå¥½ï¼Œå¯ä»¥ç”¨å®ƒåšKæŠ˜äº¤å‰éªŒè¯")
    print(f"  2. å°è¯• PointNet++ æˆ–å…¶ä»–æ¶æ„")
    print(f"  3. æ”¶é›†æ›´å¤šè®­ç»ƒæ•°æ®")
    print(f"  4. è°ƒæ•´å¢å¼ºå‚æ•°ä»¥è·å¾—æ›´å¥½çš„æ³›åŒ–")
    
    print("="*60)

if __name__ == "__main__":
    main()
