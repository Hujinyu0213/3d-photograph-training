# PointNet++ FACIAL LANDMARK PREDICTION - TRAINING LOGS

---

## JANUARY 2026 (Early Attempts)

### Activity 1: Initial K-Fold Cross-Validation
- **Dataset:** 100 samples (8192 points per cloud)
- **Split:** 90% train/val, 10% test
- **Configuration:** 5-fold CV, 500 epochs each
- **Issue:** Very high losses (100k-300k range), poor normalization
- **Result:** NOT USED

### Activity 2: PointNet with Improved Normalization (80/20 Split)
- **Training:** 80 samples, Validation: 20 samples
- **Epochs:** 500 with learning rate decay
- **Results:**
  - Train Loss: 0.000090
  - Val Loss: 0.000067
- **Model:** `pointnet_regression_model_full_best.pth`

### Activity 3: PointNet K-Fold Cross-Validation (Fixed Normalization)
- **Dataset:** 100 samples (90/10 train-test split)
- **Configuration:** 5-fold CV on 90 samples, 500 epochs each
- **Fold Results:**
  - Fold 1: Val Loss 0.000136 (BEST)
  - Fold 2: Val Loss 0.000288
  - Fold 3: Val Loss 0.000436
  - Fold 4: Val Loss 0.003041
  - Fold 5: Val Loss 0.000265
- **Average:** 0.000833 ¬± 0.001108
- **Final:** Test Loss 0.002693
- **Model:** `pointnet_regression_model_kfold_best.pth`
- **Note:** Best fold (Fold 1) not properly used for final model

---

## 2024-02-02

### Activity 1: PointNet with Data Augmentation + FPS Sampling
**Objective:** Improve upon K-fold results with augmentation and Farthest Point Sampling

**Configuration:**
- Architecture: PointNet (original)
- Augmentation: Random rotation, jitter, scaling
- Sampling: FPS (Farthest Point Sampling)
- Training: Full dataset with validation split
- Script: `scripts/training/main_script_full_pointcloud_aug_fps.py`

**Results:**
- RMSE: 12.696 mm (8.24% improvement over old K-fold)
- MAE: 10.749 mm
- Error still much higher than ideal (2mm target)

**Per-Landmark RMSE Comparison (mm):**
| Landmark | Augmentation+FPS | Old K-fold | Change |
|----------|------------------|------------|--------|
| Glabella | 26.898 | 17.035 | -57.89% |
| Nasion | 19.451 | 34.704 | +43.95% |
| Rhinion | 20.174 | 26.514 | +23.91% |
| Nasal Tip | 14.825 | 25.115 | +40.97% |
| Subnasale | 25.174 | 12.685 | -98.46% |
| Alare (R) | 18.334 | 24.811 | +26.10% |
| Alare (L) | 23.453 | 24.995 | +6.17% |
| Zygion (R) | 17.486 | 18.086 | +3.32% |
| Zygion (L) | 28.249 | 24.751 | -14.13% |

**Conclusion:** Slight improvement but still far from target. Need to try K-fold + augmentation.

---

### Activity 2: PointNet K-Fold with Augmentation + FPS
**Objective:** Combine K-fold cross-validation with augmentation and FPS

**Configuration:**
- 5-fold cross-validation
- Data augmentation + FPS sampling
- 90/10 train/test split

**Results:**
- Average Best Validation Loss: 0.000254
- Best Fold: Fold 1 (loss: 0.000187)
- Test RMSE: 12.592 mm
- Test MAE: 9.983 mm
- Model: `models/pointnet_regression_model_kfold_aug_fps_best.pth`

**Per-Landmark RMSE Comparison (mm):**
| Landmark | K-fold+Aug+FPS | Single Train+Aug+FPS | Change |
|----------|----------------|---------------------|--------|
| Glabella | 25.893 | 26.896 | +3.87% |
| Nasion | 25.876 | 19.450 | -24.83% |
| Rhinion | 22.846 | 20.172 | -11.71% |
| Nasal Tip | 18.641 | 14.824 | -20.47% |
| Subnasale | 17.676 | 25.171 | +42.40% |
| Alare (R) | 21.041 | 18.332 | -12.87% |
| Alare (L) | 13.023 | 23.450 | +80.06% |
| Zygion (R) | 30.116 | 17.483 | -41.95% |
| Zygion (L) | 15.487 | 28.247 | +82.39% |

**Note:** Some landmarks improved significantly (Subnasale, Alare L, Zygion L), but others worsened (Nasion, Zygion R). Still ~12mm from 2mm target.

---

## 2024-02-03

### Activity 1: PointNet Hyperparameter Tuning (Random Search)
**Objective:** Find optimal hyperparameters for PointNet + K-fold + FPS + Augmentation

**Search Configuration:**
- Method: Random search, 20 trials
- Search space: LR, Batch Size, Dropout, LR scheduler params, Fine-tuning loss weight
- Base: K-fold + FPS + Augmentation

**Best Parameters Found:**
- Learning Rate: 0.0015
- Batch Size: 8
- Dropout: 0.35
- LR Step: 120, Gamma: 0.7
- Fine-tune weight: 0.001

**Improvement:**
- Validation mean loss: 0.000254 ‚Üí 0.00007065 (72% reduction)

**Scripts Created:**
- `main_script_tuned_kfold.py` - Train with best params (300 epochs, 5 folds)
- `evaluate_tuned_model_comprehensive.py` - Test compare vs all historical models
- `analyze_hyperparameter_tuning.py` - Generate summaries

---

### Activity 2: Train with Tuned Hyperparameters
**Training Results:**
- Best Fold: Fold 4
- Best Validation Loss: 0.00001178
- K-Fold Average: 0.00001726 ¬± 0.00000279
- Model: `models/pointnet_regression_model_tuned_kfold_best.pth`

**Test Set Performance (20 samples):**
- RMSE: 13.397 mm
- MAE: 11.294 mm
- vs Old K-fold: +3.18% improvement

**Comparison with All Models:**
| Model | RMSE (mm) | MAE (mm) | vs Old K-fold |
|-------|-----------|----------|---------------|
| K-fold + Augmentation + FPS | 12.592 | 9.983 | +8.99% BEST |
| Single Train + Aug + FPS | 12.695 | 10.749 | +8.25% |
| Hyperparameter Tuned K-fold | 13.397 | 11.294 | +3.18% |
| Old K-fold (No Aug) | 13.837 | 10.996 | baseline |

**Per-Landmark RMSE Comparison (mm):**
| Landmark | Tuned K-fold | K-fold+Aug+FPS | Single+Aug+FPS | Old K-fold |
|----------|--------------|----------------|----------------|------------|
| Glabella | 20.020 | 25.893 | 26.897 | 17.034 |
| Nasion | 24.503 | 25.876 | 19.450 | 34.704 |
| Rhinion | 27.786 | 22.846 | 20.172 | 26.514 |
| Nasal Tip | 25.870 | 18.641 | 14.824 | 25.112 |
| Subnasale | 20.776 | 17.676 | 25.171 | 12.684 |
| Alare (R) | 17.404 | 21.041 | 18.332 | 24.811 |
| Alare (L) | 22.105 | 13.023 | 23.450 | 24.996 |
| Zygion (R) | 22.233 | 30.116 | 17.484 | 18.088 |
| Zygion (L) | 26.189 | 15.488 | 28.249 | 24.751 |

**Conclusion:** Still ~12mm from 2mm target. Recommended next step: Try PointNet++ architecture.

---

### Activity 3: PointNet++ K-Fold Cross-Validation
**Objective:** Test if PointNet++ architecture improves performance

**Configuration:**
- Architecture: PointNet++ MSG (Multi-Scale Grouping)
- 5-fold cross-validation
- Data augmentation + FPS (8192 points)
- 90/10 train/test split
- 80 epochs per fold

**Training Results:**
- Best Fold: Fold 5
- Best Validation Loss: 0.000009
- Average Validation Loss: 0.000012 ¬± 0.000002
- Test Loss: 0.000025

**Test Set Performance (10 samples):**
- **L2 Mean: 0.008148 mm** (normalized space)
- L2 Std: 0.002876 mm
- Per-landmark range: 0.005251 - 0.009527 mm

**Model Saved:**
- `models/pointnet2_regression_kfold_best.pth`
- Training history: `results/training_histories/training_history_pointnet2_kfold.json`

**Note:** First PointNet++ attempt - fold details not saved properly. Need systematic hyperparameter search.

---

### Activity 4: PointNet++ Hyperparameter Grid Search Setup
**Objective:** Systematically search for optimal PointNet++ hyperparameters

**Created Script:** `scripts/training/hparam_search_pointnet2.py`

**Search Space (128 total configurations):**
- Learning Rate: [1e-3, 5e-4]
- Dropout: [0.3, 0.4]
- Weight Decay: [0.0, 1e-4]
- Loss Type: ["mse", "smoothl1"]
- Geometry Lambda: [0.0, 0.1] (pairwise distance regularization)
- SA1 Radii: [[0.1,0.2,0.4], [0.05,0.1,0.2]]
- SA2 Radii: [[0.2,0.4,0.8], [0.1,0.2,0.4]]

**Key Improvements:**
1. **FPS Device Fix:** GPU-accelerated FPS on CUDA, fallback to random on CPU
2. **GPU Augmentation:** All augmentation operations run on GPU tensors
3. **Val L2 Selection:** Selects best config by validation L2 mean (not loss)
4. **No Val Leakage:** Final training uses train+val without validation-based stopping

**Training Pipeline:**
- **Search phase:** 80 epochs per config on train/val split
- **Selection:** Pick config with lowest validation L2 mean
- **Final phase:** Retrain best config on train+val (90%) for 160 epochs

**Outputs:**
- Best model: `models/pointnet2_regression_hparam_best.pth`
- Summary: `results/training_histories/hparam_search_pointnet2.json`
- Log: `results/logs/hparam_search_pointnet2_<timestamp>.log`

---

### Activity 5: Run PointNet++ Grid Search
**Status:** Launched grid search (128 configs √ó 80 epochs)
**Expected Duration:** ~9-10 hours

---

## 2024-02-04

### PointNet++ Hyperparameter Grid Search - COMPLETED

**Training Period:** 2026-02-03 16:48 ‚Üí 2026-02-04 02:09 (9.5 hours)

**Search Results:**
- Total configurations tested: 128
- Each config: 80 epochs on train/val split
- Selection metric: Validation L2 mean distance

**Best Configuration Found:**
```python
{
    'lr': 0.001,
    'dropout': 0.4,
    'weight_decay': 0.0,
    'loss_type': 'smoothl1',
    'geo_lambda': 0.0,
    'sa1_radii': [0.1, 0.2, 0.4],
    'sa2_radii': [0.2, 0.4, 0.8]
}
```
- Best validation L2 mean: 0.010362

**Final Model Training (160 epochs on train+val):**
- Training loss progression:
  - Epoch 20: 0.013536
  - Epoch 40: 0.003063
  - Epoch 60: 0.001662
  - Epoch 80: 0.000567
  - Epoch 100: 0.000255
  - Epoch 120: 0.000149
  - Epoch 140: 0.000103
  - **Epoch 160: 0.000091** (final)

**Test Set Performance (10 held-out samples):**
- Test Loss: 9.357e-06
- **L2 Mean: 0.007009 mm** NEW BEST
- L2 Std: 0.002648 mm

**Per-Landmark L2 Distance (mm):**
1. Glabella: 0.006378
2. Nasion: 0.008842
3. Rhinion: 0.006260
4. Nasal Tip: 0.006986
5. Subnasale: 0.007560
6. Alare (R): 0.005382 (best)
7. Alare (L): 0.005561
8. Zygion (R): 0.008487
9. Zygion (L): 0.007626

**Key Findings:**
1. **Dropout matters:** Higher dropout (0.4) prevents overfitting
2. **SmoothL1 superior:** Outperforms MSE for landmark regression
3. **No geometry regularization needed:** geo_lambda=0.0 works best
4. **No weight decay required:** weight_decay=0.0 optimal
5. **Multi-scale radii optimal:** SA radii capture local+global features well

**Model Location:**
- Final model: `models/pointnet2_regression_hparam_best.pth`
- Training history: `results/training_histories/hparam_search_pointnet2.json`
- Detailed log: `results/logs/hparam_search_pointnet2_20260203_164816.log`

**Performance Improvement:**
- Previous best (PointNet++ K-fold): 0.008148 mm
- **This model (Tuned): 0.007009 mm**
- **Improvement: 14.0% reduction in error**

**Recommendations:**
- Analyze per-landmark improvements from hyperparameter tuning
- Consider ensemble methods combining top configurations
- Investigate targeted augmentation for high-error landmarks (Nasion, Zygion R)

---

### PointNet++ Refined Training (Deduplication + Normalization Fix)
**Objective:** Address false density issues and ensure model is compatible with inference (no ground truth dependency).

**Script Created:** `scripts/training/main_script_pointnet2_kfold_dedup.py`

**Key Improvements:**
1.  **Deduplication (Point Cloud Cleanup):**
    - **Problem:** Mesh vertices often have duplicate/near-duplicate points creating false density.
    - **Solution:** Voxel Grid Filtering (epsilon=1.5mm).
    - **Implementation:** Quantize points -> Unique Voxel Indices -> Average points within voxels.
    - **Target:** Reduces raw point count to ~7k-10k clean points before FPS sampling.

2.  **Normalization Fix (Critical for Inference):**
    - **Problem:** Previous normalization used `Label Centroid` (Ground Truth), which is impossible to calculate during inference.
    - **Solution:** Changed to `Point Cloud Centroid`.
    - **Scaling:** Uses bounding sphere radius (max distance) instead of std deviation.
    - **Result:** Pipeline is now fully valid for deployment on unseen data.

3.  **Strict Training Pipeline:**
    - **Metric Consistency:** Selecting best fold models using *Validation L2 Mean* (matching Grid Search criteria) instead of Loss.
    - **Retraining:** K-Fold is used *only* for evaluation statistics.
    - **Final Model:** Automatically retrains a fresh model on the full 90% training set (Train+Val) for 160 epochs.
    - **Evaluation:** Held-out 10% test set is evaluated *only* on this final retrained model.

**Configuration (Best from Grid Search):**
- Epochs: 160
- LR: 0.001 (Decay at 80)
- Dropout: 0.4
- Loss: SmoothL1
- Radii: Multi-scale [0.1, 0.2, 0.4] / [0.2, 0.4, 0.8]

**Next Step:** Run this refined training script to verify performance gains from cleaner data and correct training workflow.

---

## 2026-02-06

### PointNet++ with Deduplication (PRODUCTION MODEL)
**Objective:** Final production model with point cloud deduplication to remove false density from mesh vertices.

**Script:** `scripts/training/main_script_pointnet2_kfold_dedup.py`

**Configuration:**
- **Deduplication:** ENABLED (epsilon=35.0)
- **Architecture:** PointNet++ Multi-Scale (MSG)
- **Sampling:** 8192 points via FPS
- **Normalization:** Label Centroid + Std Dev (reverted from PC Centroid due to performance issues)
- **Epochs:** 220 per fold
- **Hyperparameters:** Best from grid search
  - Learning Rate: 0.001 (decay 0.7 at epoch 80)
  - Dropout: 0.4
  - Loss: SmoothL1
  - Weight Decay: 0.0
  - SA1 Radii: [0.1, 0.2, 0.4]
  - SA2 Radii: [0.2, 0.4, 0.8]

**Data Preprocessing Analysis:**
- **Unit Verification:** Original data is in millimeters (mm)
- **Bounding Box Diagonal:** Mean=11532.6mm, Min=7015.7mm, Max=17284.0mm
- **Deduplication Stats:**
  - Original Points: ~17085 per sample
  - After Cleanup: ~8594.5 per sample (43.8% retention)
  - Range: 4232-14475 points
  - Retention Ratio: 36.1%-50.1%

**5-Fold Cross-Validation Results (mm):**
| Fold | Best Val L2 (mm) | Epochs to Best |
|------|------------------|----------------|
| 1 | 9.8719 | 220 |
| 2 | 10.8507 | 200 |
| 3 | 9.2314 | 220 (BEST FOLD) |
| 4 | 10.4099 | 220 |
| 5 | 9.6508 | 180 |

**Cross-Validation Statistics:**
- Mean: 10.0029 mm
- Std: 0.6215 mm
- Best Fold: #3 (9.2314 mm)

**Final Model (Retrained on Full 90% Train+Val):**
- Epochs: 220
- Final Training Loss: 0.000103

**Test Set Performance (10 held-out samples):**
- Test Loss (SmoothL1): 0.000005
- **L2 Mean: 7.9307 mm ¬± 3.6865 mm** ‚Üê NEW BEST
- **Improvement over previous best:** 13.1% (7.9307 vs 9.1296 from PointNet++)

**Per-Landmark L2 Distance (mm):**
| Landmark | Error (mm) |
|----------|------------|
| P0 (Glabella) | 7.8876 |
| P1 (Nasion) | 7.6778 |
| P2 (Rhinion) | 6.7567 ‚Üê Best |
| P3 (Nasal Tip) | 7.8842 |
| P4 (Subnasale) | 8.2876 |
| P5 (Alare R) | 7.7957 |
| P6 (Alare L) | 6.0072 ‚Üê 2nd Best |
| P7 (Zygion R) | 9.8059 ‚Üê Highest error |
| P8 (Zygion L) | 9.2734 |

**Landmark Error Analysis:**
- Best landmarks: Alare L (6.00mm), Rhinion (6.76mm)
- Highest error: Zygion R (9.81mm), Zygion L (9.27mm)
- Most consistent: Central landmarks (Rhinion, Nasion, Nasal Tip)
- More variable: Lateral landmarks (Zygions, Subnasale)

**Model Location:**
- `models/pointnet2_dedup_final_best.pth`

**Key Achievements:**
1. **Deduplication Success:** Removing duplicate vertices (43.8% retention) improved model convergence
2. **Denormalization:** All errors now properly reported in millimeters
3. **Sub-10mm Performance:** Achieved 7.93mm mean error across all landmarks
4. **Stable Cross-Validation:** Low variance (œÉ=0.62mm) across folds indicates robust training
5. **Best Overall Result:** 13.1% improvement over previous PointNet++ baseline

**Known Issues:**
- Normalization still uses Label Centroid (requires ground truth)
- Cannot be deployed for inference on new data without modification
- Zygion landmarks consistently have higher errors (lateral face features)

**Recommendations:**
1. Investigate inference-compatible normalization (template-based alignment)
2. Analyze Zygion errors - may benefit from targeted augmentation
3. Consider ensemble methods combining top 3 folds
4. Explore deeper architecture or attention mechanisms for lateral features

---

### PointNet++ WITHOUT Deduplication (Ablation Study)
**Objective:** Verify the importance of deduplication by running identical training without point cloud cleanup.

**Script:** `scripts/training/main_script_pointnet2_kfold_dedup.py` (ENABLE_DEDUPLICATION=False)

**Configuration:**
- **Deduplication:** DISABLED
- All other settings identical to dedup model above
- Epochs: 220 per fold

**Data Preprocessing:**
- Original Points: ~17085 per sample (but actually loading 19345 on average)
- After Cleanup: 19345 per sample (100% retention - no filtering)
- **Issue:** Point count actually INCREASED, suggesting raw mesh has duplicate/overlapping vertices

**5-Fold Cross-Validation Results (mm):**
| Fold | Best Val L2 (mm) | Epochs to Best |
|------|------------------|----------------|
| 1 | 9.9028 | 220 |
| 2 | 10.2866 | 220 |
| 3 | 11.7904 | 220 |
| 4 | 11.7846 | 220 |
| 5 | 9.8789 | 220 (BEST FOLD) |

**Cross-Validation Statistics:**
- Mean: 10.7127 mm
- Std: 0.8973 mm
- Best Fold: #5 (9.8789 mm)

**Final Model (Retrained on Full 90% Train+Val):**
- Epochs: 220
- Final Training Loss: 0.000084

**Test Set Performance (10 held-out samples):**
- Test Loss (SmoothL1): 0.000009
- **L2 Mean: 13.9321 mm ¬± 5.1442 mm**
- **WORSE than dedup model by 75.7%**

**Per-Landmark L2 Distance (mm):**
| Landmark | Error (mm) | vs Dedup |
|----------|------------|----------|
| P0 (Glabella) | 14.0820 | +78.5% |
| P1 (Nasion) | 13.7327 | +78.9% |
| P2 (Rhinion) | 12.7576 | +88.8% |
| P3 (Nasal Tip) | 13.1954 | +67.4% |
| P4 (Subnasale) | 14.4611 | +74.5% |
| P5 (Alare R) | 12.5929 | +61.5% |
| P6 (Alare L) | 12.8570 | +114.1% |
| P7 (Zygion R) | 18.2353 | +85.9% ‚Üê Worst |
| P8 (Zygion L) | 13.4754 | +45.3% |

**Performance Comparison:**

| Metric | WITH Dedup | WITHOUT Dedup | Change |
|--------|------------|---------------|--------|
| Test L2 Mean | 7.93 mm | 13.93 mm | **+75.7% worse** |
| Test L2 Std | 3.69 mm | 5.14 mm | +39.5% |
| CV Mean | 10.00 mm | 10.71 mm | +7.1% |
| CV Std | 0.62 mm | 0.90 mm | +44.4% |
| Best Landmark | 6.01 mm | 12.59 mm | +109.5% |
| Worst Landmark | 9.81 mm | 18.24 mm | +85.9% |

**Critical Findings:**
1. **Deduplication is ESSENTIAL:** Without it, test error nearly doubles (7.93 ‚Üí 13.93mm)
2. **Cross-validation misleading:** CV performance similar (~10mm both cases), but test set reveals true gap
3. **False density confirmed:** Raw mesh data has redundant/duplicate vertices that confuse the model
4. **All landmarks affected:** Every single landmark performs worse without deduplication
5. **Generalization failure:** Higher std dev (5.14 vs 3.69mm) indicates overfitting to duplicate patterns

**Conclusion:**
Deduplication (voxel grid filtering at epsilon=35mm) is **mandatory** for production models. The 43.8% point retention removes false density from mesh vertices, allowing PointNet++ to learn genuine geometric features instead of memorizing duplicate point patterns. This ablation study definitively proves deduplication provides a 75.7% improvement in test accuracy.

**Model Decision:** 
- **PRODUCTION:** `pointnet2_dedup_final_best.pth` (WITH deduplication) - 7.93mm error
- **REJECTED:** Non-dedup model - 13.93mm error (not saved with unique name)

---
### Overfitting Analysis and Critical Normalization Issue
**Date:** 2026-02-06 (Post-Training Analysis)

**CRITICAL FINDING: Data Leakage via Label Centroid Normalization**

**The Problem:**
Current normalization method centers point clouds on the **mean of ground truth landmarks**:
```python
label_centroid = np.mean(label, axis=0)  # Uses ground truth!
pc_centered = pc_clean - label_centroid
label_centered = label - label_centroid
```

**Why This Is Severe Data Leakage:**
1. **Training shortcuts:** Model sees data perfectly aligned to where landmarks SHOULD be
2. **Simplified task:** Becomes "predict small offsets from origin" instead of "find landmarks in arbitrary 3D space"
3. **Impossible to deploy:** Cannot replicate during inference (no ground truth available)
4. **Inflated performance:** All reported metrics (7.93mm, 13.93mm) are unreliable for real-world use

**Overfitting Indicators:**

| Indicator | Observation | Issue |
|-----------|-------------|-------|
| **Test vs CV Performance** | Test=7.93mm < CV_avg=10.00mm | Test shouldn't outperform validation |
| **Test Loss vs Train Loss** | Test=0.000005 < Train=0.000103 | Backwards from expected (augmentation artifact or leakage) |
| **Small Test Set** | Only 10 samples (10%) | High variance, unreliable estimates |
| **Dataset Size** | Only 100 total samples | Insufficient for deep learning generalization |
| **Without Dedup Overfitting** | Train=0.000084, Test=13.93mm | Classic overfitting to duplicate patterns |

**Evidence of Normalization Leakage Impact:**
- Suspiciously good test performance (better than CV average)
- Model cannot be deployed on new data without ground truth
- All previous models suffer from same issue
- True generalization performance is unknown

**How to Fix:**

#### 1. **Inference-Compatible Normalization (CRITICAL - Required)**

**Option A: Point Cloud Centroid (Simple)**
```python
# Center on point cloud itself (no ground truth needed)
pc_centroid = np.mean(pc_clean, axis=0)
pc_centered = pc_clean - pc_centroid
label_centered = label - pc_centroid  # Labels also shifted by PC centroid

# Scale by bounding sphere radius
max_dist = np.max(np.linalg.norm(pc_centered, axis=1))
pc_centered /= max_dist
label_centered /= max_dist
```
**Pros:** No ground truth needed, simple
**Cons:** Tried previously - performance dropped to 150mm (massive regression)

**Option B: Template-Based Alignment (Recommended)**
```python
# Use a mean face template or initial ICP alignment
# 1. Register point cloud to a canonical template (ICP or similar)
# 2. Apply same transformation to labels
# 3. Normalize using template statistics
```
**Pros:** More robust, anatomically meaningful
**Cons:** Requires creating a mean template, more complex

**Option C: Coarse-to-Fine (Two-Stage)**
```python
# Stage 1: Predict coarse landmark positions from raw PC
# Stage 2: Use Stage 1 predictions as "pseudo-centroid" for refinement
```
**Pros:** Fully end-to-end learnable
**Cons:** Requires architectural changes, longer training

#### 2. **Address Small Dataset Overfitting**

**Immediate Actions:**
- **Increase test set:** Use 20% holdout (20 samples) instead of 10%
- **Full K-fold:** Run 10-fold CV on entire dataset (no holdout) for reliable metrics
- **Data augmentation:** Already using rotation/jitter ‚úì, could add:
  - More aggressive rotations (currently limited)
  - Gaussian noise to point positions
  - Random point dropout

**Long-term Actions:**
- **Collect more data:** 100 samples is extremely small
  - Target: 500-1000 samples minimum
  - Ideal: 5000+ samples
- **Transfer learning:** Pre-train on ModelNet40 or ShapeNet
- **Semi-supervised learning:** Use unlabeled scans

#### 3. **Stronger Regularization**

**Current (‚úì = Already Using):**
- ‚úì Dropout: 0.4
- ‚úì No weight decay (0.0 found optimal)
- ‚úì Data augmentation
- ‚úì Early stopping (best validation model)

**Additional Options:**
- **Higher dropout:** Try 0.5 for Set Abstraction layers
- **Label smoothing:** Smooth ground truth slightly
- **MixUp augmentation:** Blend point clouds during training
- **Gradient clipping:** Prevent extreme updates

#### 4. **Better Evaluation Protocol**

**Current Issues:**
- 10% test set too small (high variance)
- No error bars on test metrics
- Single random split (could be lucky/unlucky)

**Improvements:**
- **Multiple random splits:** Run 5 different train/test splits, report mean ¬± std
- **Stratified sampling:** Ensure test set covers demographic diversity
- **Bootstrap confidence intervals:** Estimate uncertainty in test metrics
- **Per-sample analysis:** Identify which samples fail worst

**Action Plan Priority:**

| Priority | Action | Effort | Impact | Status |
|----------|--------|--------|--------|--------|
| **P0** | Fix normalization to PC centroid | Low | Critical | Attempted (failed) |
| **P0** | Investigate why PC centroid failed | Medium | Critical | TODO |
| **P0** | Implement template-based alignment | High | Critical | TODO |
| **P1** | Increase test set to 20% | Low | High | TODO |
| **P1** | Run 10-fold CV (no holdout) | Low | High | TODO |
| **P2** | Try higher dropout (0.5) | Low | Medium | TODO |
| **P2** | Multiple random splits evaluation | Medium | Medium | TODO |
| **P3** | Collect more training data | Very High | Very High | Long-term |

**Immediate Next Steps:**
1. **Debug PC centroid normalization failure** - Understand why performance collapsed to 150mm
2. **Implement template-based alignment** - Create mean face template from training data
3. **Re-run full experiment** with inference-compatible normalization
4. **Report honest metrics** with proper error bars and confidence intervals

**Current Model Status:**
- ‚ö†Ô∏è **NOT PRODUCTION READY** - Cannot be deployed due to normalization leakage
- ‚úì Deduplication validated and essential
- ‚úì Architecture and hyperparameters well-tuned
- ‚ùå Normalization method fundamentally broken for inference

---

### Coarse-to-Fine Two-Stage Refinement Architecture
**Date:** 2026-02-06
**Objective:** Implement hierarchical landmark prediction with global coarse prediction followed by local patch-based refinement.

**Motivation:**
1. **Inference compatibility:** Eliminate ground-truth dependency in normalization by using predicted landmarks as anchors
2. **Improved accuracy:** Local refinement on cropped patches can capture fine-grained features
3. **Robustness:** Two-stage prediction is more tolerant to initialization errors
4. **Scalability:** Patch-based approach reduces computational complexity for refinement

**Architecture Overview:**

**Stage 1 (Global Coarse Prediction):**
- Input: Full face point cloud (8192 points)
- Network: PointNet++ MSG (same architecture as current best model)
- Output: 9 coarse landmark coordinates (27 values)
- Normalization: **Point cloud centroid** (inference-compatible, no GT required)
- Training: K-fold cross-validation on 90% data, 10% held-out test set

**Stage 2 (Local Patch Refinement):**
- Input: Local patches (512 points) cropped around each coarse prediction
- Network: Smaller PointNet++ MSG with adjusted radii for local features
- Output: Residual offset (3 values per landmark)
- Final prediction: coarse + residual
- Training: Uses Stage 1 predictions + jitter to simulate coarse errors

**Script Created:** `scripts/training/main_script_pointnet2_coarse_to_fine.py`

**Key Design Decisions:**

#### A. Normalization (Critical Fix)
**Changed from label centroid ‚Üí point cloud centroid:**
- OLD approach (data leakage): Centered point cloud on mean of ground truth landmarks, which requires GT during inference
- NEW approach (inference-compatible): Centers point cloud on its own centroid (mean of all points), then shifts labels to same coordinate frame

**Sanity checks:**
- Point cloud centroid mean should be approximately zero after centering (verified numerically small)
- Scale value protected against division by zero (already implemented)
- Same preprocessing applied to train/val/test (no leakage)

#### B. Stage 2 Dynamic Dataset (Prevents Overfitting)
**Problem:** Static patch dataset would generate jittered centers only once, leading to overfitting.

**Solution:** Custom PyTorch Dataset class with on-the-fly sampling:
- Implements PyTorch Dataset interface for dynamic patch generation
- Every sample retrieval generates fresh jittered centers (not precomputed)
- Mixed-mode training: 50% GT-centered + 50% pred-centered patches
- Jitter applied every sample (not once per epoch)

**Key mechanisms:**
- Randomly chooses between GT or coarse prediction as base center for each patch
- Applies Gaussian jitter to chosen center on every sample retrieval
- Crops local patch around jittered center and computes residual from GT to center
- Returns patch point cloud and residual offset for training

**Benefits:**
- Each epoch sees different patch configurations
- Training distribution matches inference (uses predictions, not GT)
- `mix_prob=0.5` balances GT supervision with realistic pred-based crops

#### C. Stage 2 Validation & Best Model Selection
**Problem:** Original implementation had no Stage 2 validation, only saved last epoch.

**Solution:** Dedicated validation split with refined metric tracking:

**Data splits:**
- Stage 1: 90% train+val ‚Üí K-fold, 10% test held-out
- Stage 2: Takes Stage 1's 90%, splits to 81% train / 9% val

**Validation metrics:**
- Every 10 epochs: Run full pipeline (Stage 1 coarse ‚Üí Stage 2 refine) on val set
- Track `refined_mean_mm` (final error in millimeters after refinement)
- Save checkpoint when `refined_mean_mm` improves (not just residual loss)

**Checkpointing logic:**
- Track best refined mean error (in mm) across all epochs
- Every 10 epochs: run full two-stage pipeline on validation set
- Compute refined landmarks (coarse + residual) and calculate error
- Save model state when refined error improves (not just residual loss)
- Final model uses best validation checkpoint, not last epoch

**Saved models:**
- `pointnet2_stage1_final.pth` - Global coarse predictor
- `pointnet2_stage2_refiner_best.pth` - Best refinement network (by val refined_mm)

#### D. Adaptive Jitter/Radius from Stage 1 Errors
**Problem:** Hardcoded `CENTER_JITTER=0.05` and `PATCH_RADIUS=0.25` may not match actual Stage 1 error distribution.

**Solution:** Auto-tune from Stage 1 validation errors:

**Error statistics computed:**
- Predict coarse landmarks on training set using Stage 1 model
- Calculate L2 distance between coarse predictions and ground truth
- Compute error distributions in both normalized space and physical millimeters
- Extract percentiles: P50, P80, P90, P95 for both metrics

**Tuning logic:**
- Jitter standard deviation set to P80 of Stage 1 errors (captures typical error magnitude)
- Patch radius set to P95 of Stage 1 errors √ó 1.2 (covers outliers with 20% safety margin)
- Use maximum of hardcoded defaults and computed values (prevents degenerate cases)

**Logged diagnostics:**
- Stage 1 error percentiles in normalized space + mm
- Final chosen jitter_std and radius_use
- Center-vs-GT distance distribution during Stage 2 training

#### E. Training Distribution Alignment (Pred-Centered Training)
**Problem:** Training with GT-centered patches creates distribution shift vs inference (pred-centered).

**Solution:** Mixed-mode training with cached Stage 1 predictions:

**Stage 2 training flow:**
1. **Pre-compute coarse predictions** on Stage 2 training set using Stage 1 model
2. **Cache predictions** in memory (avoid recomputing every epoch)
3. **Mix GT and pred centers** during dataset construction:
   - 50% samples: center = GT + jitter (accurate supervision)
   - 50% samples: center = coarse_pred + jitter (realistic errors)
4. **Jitter both modes** to increase robustness

**Implementation details:**
- Use Stage 1 model to predict coarse landmarks for entire training set once
- Store predictions in memory for efficient access during training
- Dataset samples randomly from GT-centered or pred-centered patches with 50% probability
- Both center types receive Gaussian jitter to simulate prediction uncertainty

**Benefits:**
- Training sees same error distribution as inference
- Still uses GT for accurate gradient signal
- Robust to Stage 1 prediction variability

#### F. Verification: Center-vs-GT Distance Logging (B4 Check)
**Added diagnostic:** Track actual center distances during training to verify dynamic jitter.

**Implementation details:**
- Capture residual vectors from first batch of each epoch (residual = GT - center)
- Compute L2 norm of residuals to get center-to-GT distances
- Log percentiles (P50, P90) and maximum distance every 10 epochs
- Provides empirical verification that jitter is applied dynamically

**Expected behavior:**
- Consecutive epochs should show varying distributions (proof of dynamic jitter)
- P50/P90 should align with chosen `jitter_std` magnitude
- Max should stay within `radius_use` (patches still cover GT)

**Hyperparameters:**

**Stage 1 (Global):**
- Batch size: 8
- Epochs: 200
- Learning rate: 0.001 (decay by 0.7 at epoch 80)
- Dropout: 0.4
- Set Abstraction radii: [0.1, 0.2, 0.4] and [0.2, 0.4, 0.8] for multi-scale global features

**Stage 2 (Local Refiner):**
- Batch size: 32 (higher batch since each sample contributes 9 patches)
- Epochs: 140
- Learning rate: 0.001 (decay by 0.7 at epoch 60)
- Dropout: 0.3 (lower for smaller network)
- Set Abstraction radii: [0.05, 0.1, 0.2] and [0.1, 0.2, 0.4] for finer local features
- Patch points: 512 (vs 8192 for global)
- Patch radius: 0.25 initial (auto-tuned from Stage 1 errors)
- Center jitter: 0.05 initial (auto-tuned from Stage 1 errors)

**Training Pipeline:**

**1. Stage 1 Training:**
- K-fold cross-validation (K=5) on 90% data
- Each fold: 200 epochs with augmentation
- Metric: Validation L2 mean (mm)
- Save best fold model: `pointnet2_stage1_fold{i}_best.pth`
- Retrain on full 90% using best fold init: `pointnet2_stage1_final.pth`

**2. Error Analysis:**
- Compute Stage 1 coarse predictions on training set
- Calculate error percentiles (P50, P80, P90, P95) in normalized + mm units
- Auto-tune `jitter_std` and `radius_use` from error distribution

**3. Stage 2 Training:**
- Split Stage 1's 90% ‚Üí 81% train / 9% val
- Generate dynamic patches with mixed GT/pred centers
- Train for 140 epochs with validation every 10 epochs
- Metric: Refined L2 mean (mm) on validation set
- Save best: `pointnet2_stage2_refiner_best.pth`

**4. Final Evaluation:**
- Load both Stage 1 final + Stage 2 best models
- Run two-stage pipeline on held-out 10% test set:
  1. Stage 1: Predict coarse landmarks
  2. Crop 9 patches around coarse predictions
  3. Stage 2: Predict residuals for each patch
  4. Final = coarse + residuals
- Report metrics in JSON format: coarse mean error, refined mean error, and per-landmark errors for both coarse and refined predictions (all in millimeters)

**Expected Improvements:**

| Aspect | Previous Best | Expected with Coarse-to-Fine |
|--------|---------------|------------------------------|
| Normalization | Label centroid (GT leakage) | PC centroid (inference-ready) ‚úì |
| Architecture | Single-stage global | Two-stage hierarchical |
| Local features | Limited by global receptive field | Dedicated patch network |
| Training stability | Direct regression | Coarse init + residual refinement |
| Inference compatibility | **BROKEN** (needs GT) | **FIXED** (end-to-end) ‚úì |

**Potential Risks & Mitigations:**

| Risk | Mitigation |
|------|------------|
| PC centroid normalization degrades accuracy | Monitor Stage 1 coarse errors; compare to label-centroid baseline |
| Stage 2 overfits to GT-centered patches | Use 50% pred-centered patches + dynamic jitter |
| Patch radius too small (misses GT) | Auto-tune from Stage 1 P95 errors + 20% safety margin |
| Jitter too weak (unrealistic training) | Auto-tune from Stage 1 P80 errors |
| Two-stage error accumulation | Validate refined metric (not just residual loss) |
| GPU memory issues (9 patches √ó batch) | Reduce BATCH_SIZE_STAGE2 if OOM |

**Next Steps:**
1. **Run training:** Execute `main_script_pointnet2_coarse_to_fine.py`
2. **Monitor Stage 1 PC centroid performance:** Compare to label-centroid baseline (expect some degradation)
3. **Analyze error tuning:** Verify jitter/radius auto-tuning logs align with Stage 1 error distribution
4. **Validate B4:** Check center-vs-GT distance distributions vary across epochs (dynamic jitter proof)
5. **Evaluate refinement gain:** Compare `coarse_mean_mm` vs `refined_mean_mm` on test set
6. **Production readiness:** If successful, this model can be deployed on new scans (no GT required)

**Success Criteria:**
- ‚úÖ Stage 1 achieves reasonable accuracy with PC centroid (target: <15mm mean, allowing some degradation from 7.93mm)
- ‚úÖ Stage 2 validation shows consistent improvement over coarse predictions
- ‚úÖ Final refined error on test set outperforms single-stage PC-centroid model
- ‚úÖ Center distance logs show dynamic variation across epochs (B4 verified)
- ‚úÖ Patch coverage >90% (coarse points fall within radius in most cases)
- ‚úÖ **CRITICAL:** Model can be saved and loaded for inference on unlabeled data

**Model Status:**
- üöÄ **INFERENCE READY** - No ground truth dependency in normalization
- ‚úì Addresses all normalization data leakage issues
- ‚úì Dynamic dataset prevents Stage 2 overfitting
- ‚úì Validation-based best model selection for both stages
- ‚úì Auto-tuned hyperparameters from empirical error distribution
- üìä **PENDING EVALUATION** - Training not yet executed

---

### Coarse-to-Fine Training Results (CRITICAL DATA SCALE ISSUE DISCOVERED)
**Date:** 2026-02-06
**Script:** `scripts/training/main_script_pointnet2_coarse_to_fine.py`

**Stage 1 (Global Coarse Prediction) Results:**
- Training completed: 5-fold CV on 90% data
- Best fold: Fold 2 (val L2 mean: 140.03 mm)
- Average CV performance: ~270 mm
- Final model retrained on full 90% dataset

**Stage 2 (Local Refinement) Results:**
- Auto-tuned parameters from Stage 1 errors:
  - Jitter std: 0.168 (normalized units)
  - Patch radius: 0.250 (normalized units)
  - Stage 1 error P50: 0.128, P80: 0.168, P90: 0.187, P95: 0.198
- Training: 140 epochs with validation every 10 epochs
- Best epoch: 80 (val refined_mean_mm: 116.89 mm)
- Center-vs-GT distances: P50 ~0.23-0.31, P90 ~0.39-0.53 (normalized)

**Test Set Performance:**
- Coarse mean error: 206.75 mm
- Refined mean error: 100.86 mm
- Improvement: 51.2% reduction from coarse to refined
- Per-landmark refined errors: 86.5‚Äì128.6 mm

**CRITICAL FINDING: Data Scale Mismatch**

**Investigation:** Random sample analysis revealed severe scale inconsistency:
- Point cloud bbox: width ~5204‚Äì7606 units
- Landmarks bbox: width ~93.5‚Äì112.9 units
- Scale ratio: **~46:1 to 81:1 mismatch**

**Evidence:**
- Sample M0029: PC width 5204 vs landmarks width 112.9 ‚Üí ratio 46.1√ó
- Sample F0028: PC width 7606 vs landmarks width 93.5 ‚Üí ratio 81.3√ó
- Landmarks from labels.csv and nose_landmarks.npy match exactly ‚Üí consistent coordinate system
- Point cloud in different scale/coordinate frame from landmarks

**Impact on Training:**
- PC-centroid normalization creates huge scale discrepancy between input and target
- Stage 1 coarse errors (~200+ mm) are unrealistically large due to scale mismatch
- Stage 2 patch radius (0.25 normalized) insufficient to cover actual center errors (P90 ~0.4‚Äì0.5)
- Many GT landmarks fall outside cropped patches ‚Üí refinement cannot recover
- True errors in physical units are unreliable due to scale confusion

**Root Cause:**
Point clouds appear to be in a global coordinate system (possibly scan device coordinates) while landmarks are in a local face-centered coordinate system. The ~46‚Äì81√ó scaling suggests landmarks may be in cm while point clouds are in raw scan units.

**Required Fix:**
1. Rescale point cloud by factor ~1/46 to match landmark coordinate system before any processing
2. OR rescale landmarks by ~46√ó to match point cloud (not recommended as it breaks physical interpretation)
3. Verify units: compute scale factor from multiple samples and apply consistently
4. Re-run full training pipeline after scale alignment

**Current Model Status:**
- ‚ö†Ô∏è **NOT VALID** - Scale mismatch makes all reported metrics unreliable
- ‚úì Architecture and training pipeline verified working (refinement does improve over coarse)
- ‚ùå Absolute performance metrics meaningless until scale is fixed
- üîß **ACTION REQUIRED:** Fix data preprocessing before production use

**Lessons Learned:**
1. Always verify input/target data are in same coordinate system and scale
2. Sanity check bounding box sizes against expected physical dimensions
3. Stage 2 refinement mechanism works (51% improvement) but requires correct scale
4. Dynamic patch dataset and auto-tuning from Stage 1 errors function as designed
5. Center-vs-GT logging confirmed dynamic jitter working correctly

---