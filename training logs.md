================================================================================
TRAINING LOGS - SUMMARY
================================================================================

**1. Early K-Fold Cross-Validation (Initial Attempt)**
   - Dataset: 100 samples (8192 points per cloud)
   - Split: 90% train/val, 10% test (10 samples)
   - 5-fold CV, 500 epochs each
   - **Issue:** Very high losses (100k-300k range), poor normalization
   - Result: NOT USED

**2. PointNet with Improved Normalization (80/20 Split)**
   - Training: 80 samples, Validation: 20 samples
   - 500 epochs, learning rate decay
   - Final: Train Loss 0.000090, Val Loss 0.000067
   - Model: `pointnet_regression_model_full_best.pth`

**3. PointNet K-Fold Cross-Validation (Fixed Normalization)**
   - Dataset: 100 samples (90/10 train-test split)
   - 5-fold CV on 90 samples, 500 epochs each
   - **Fold Results:**
     * Fold 1: Val Loss 0.000136 â­ (BEST)
     * Fold 2: Val Loss 0.000288
     * Fold 3: Val Loss 0.000436
     * Fold 4: Val Loss 0.003041
     * Fold 5: Val Loss 0.000265
   - Average: 0.000833 Â± 0.001108
   - Final retrain on 90 samples â†’ Test Loss: 0.002693
   - Model: `pointnet_regression_model_kfold_best.pth`
   - **Note:** Best fold (Fold 1) not properly used for final model



============================================================

ğŸ§ª åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æœ€ç»ˆæ¨¡å‹

============================================================

æµ‹è¯•é›†å¤§å°: 10 ä¸ªæ ·æœ¬

æµ‹è¯•é›†æŸå¤±: 0.002693



ğŸ‰ KæŠ˜äº¤å‰éªŒè¯ + æœ€ç»ˆæ¨¡å‹è®­ç»ƒå®Œæˆï¼

Â   KæŠ˜äº¤å‰éªŒè¯: è®­ç»ƒäº† 5 ä¸ªæ¨¡å‹ç”¨äºé€‰æ‹©æœ€ä½³é…ç½®

Â   å¹³å‡éªŒè¯æŸå¤±: 0.000833 Â± 0.001108

Â   æœ€ä½³æŠ˜: æŠ˜ 1ï¼ŒéªŒè¯æŸå¤±: 0.000136

Â   æœ€ç»ˆæ¨¡å‹: ç”¨æ‰€æœ‰ 90 ä¸ªæ ·æœ¬é‡æ–°è®­ç»ƒ

Â   â­ æµ‹è¯•é›†æŸå¤±: 0.002693 (ç‹¬ç«‹è¯„ä¼°ï¼Œæ— å)



ğŸ“ æœ€ç»ˆæ¨¡å‹æ–‡ä»¶: C:\\Users\\mkale\\Desktop\\Pointnet\_Pointnet2\_pytorch-master\\PointFeatureProject\\pointnet\_regression\_model\_kfold\_best.pth





================================================================================#### 2/2 note hu



**1. Tried to train with FPS sampling and add the validation set and data enhancing. The error is still huge compared to the ideal value 2mm, the full result compared to the kfold model are as below. The script is : scripts/training/main\_script\_full\_pointcloud\_aug\_fps.py**



============================================================

ğŸ“Š æµ‹è¯•é›†æ€§èƒ½å¯¹æ¯” (æ¯«ç±³)

============================================================



æ•´ä½“æŒ‡æ ‡å¯¹æ¯” (æ¯«ç±³):

æŒ‡æ ‡                            å¢å¼º+FPS            KæŠ˜æ¨¡å‹         æ”¹è¿›

---

MSE(mm^2)                 161.196320      191.456848     15.81%

RMSE(mm)                   12.696311       13.836793      8.24%

MAE(mm)                    10.749276       10.996382      2.25%



æ¯ä¸ªåœ°æ ‡ç‚¹3Dè¯¯å·®å¯¹æ¯” (RMSE, mm):

åœ°æ ‡ç‚¹                   å¢å¼º+FPS         KæŠ˜æ¨¡å‹         æ”¹è¿›

---

Glabella           26.898201    17.035503    -57.89%

Nasion             19.451246    34.704235     43.95%

Rhinion            20.173586    26.513836     23.91%

Nasal Tip          14.824703    25.115013     40.97%

Subnasale          25.173956    12.684813    -98.46%

Alare (R)          18.334316    24.811174     26.10%

Alare (L)          23.452953    24.995451      6.17%

Zygion (R)         17.486124    18.085920      3.32%

Zygion (L)         28.249075    24.751223    -14.13%



ğŸ“ è¯¦ç»†ç»“æœå·²ä¿å­˜: C:\\Users\\mkale\\Desktop\\Pointnet\_Pointnet2\_pytorch-master\\PointFeatureProject\\results\\test\_comparison\_aug\_fps\_vs\_old\_kfold.json



============================================================

ğŸ’¡ ç»“è®ºä¸å»ºè®®

============================================================

âœ… å¢å¼º+FPSæ¨¡å‹ç•¥ä¼˜äºKæŠ˜æ¨¡å‹ (RMSEæ”¹è¿› 8.24%)

Â   å»ºè®®: å¯ä»¥ä½¿ç”¨å¢å¼º+FPSæ¨¡å‹ï¼Œæˆ–ç»“åˆä¸¤è€…ä¼˜ç‚¹



ä¸‹ä¸€æ­¥:

Â  1. å¦‚æœå¢å¼º+FPSæ•ˆæœå¥½ï¼Œå¯ä»¥ç”¨å®ƒåšKæŠ˜äº¤å‰éªŒè¯

Â  2. å°è¯• PointNet++ æˆ–å…¶ä»–æ¶æ„

Â  3. æ”¶é›†æ›´å¤šè®­ç»ƒæ•°æ®

Â  4. è°ƒæ•´å¢å¼ºå‚æ•°ä»¥è·å¾—æ›´å¥½çš„æ³›åŒ–

============================================================



**2. Try to combine k-fold with all these changes above.**

**æ–° KæŠ˜æ¨¡å‹åœ¨éƒ¨åˆ†ç‚¹ä¸Šæœ‰æ˜¾è‘—æ”¹è¿›ï¼ˆSubnasale, Alare L, Zygion Lï¼‰ï¼Œä½†åœ¨å¦ä¸€äº›ç‚¹åè€Œå˜å·®ï¼ˆNasion, Zygion Rï¼‰**

**æ•´ä½“ RMSE/MAE ç•¥æœ‰æ”¹è¿›ï¼Œä½†ä¸ç¨³å®šï¼Œè¯´æ˜éœ€è¦æ›´å¤šæ•°æ®æˆ–è¿›ä¸€æ­¥è°ƒä¼˜**

**è·ç¦» 2mm ç›®æ ‡è¿˜å¾ˆè¿œï¼ˆå½“å‰çº¦ 12-13mmï¼‰ï¼Œéœ€è¦æ”¶é›†æ›´å¤šæ ·æœ¬æˆ–å°è¯• PointNet++**



ğŸ“¦ KæŠ˜è®­ç»ƒå®Œæˆ

å¹³å‡æœ€ä½³éªŒè¯æŸå¤±: 0.0002540331498797362

æœ€ä½³æŠ˜: 1 loss: 0.00018740897454942265

æœ€ä½³æ¨¡å‹: C:\\Users\\mkale\\Desktop\\Pointnet\_Pointnet2\_pytorch-master\\PointFeatureProject\\models\\pointnet\_regression\_model\_kfold\_aug\_fps\_best.pth

è®­ç»ƒå†å²: C:\\Users\\mkale\\Desktop\\Pointnet\_Pointnet2\_pytorch-master\\PointFeatureProject\\results\\training\_history\_kfold\_aug\_fps.json







============================================================

ğŸ“Š æµ‹è¯•é›†æ€§èƒ½å¯¹æ¯” (æ¯«ç±³)

============================================================



æ•´ä½“æŒ‡æ ‡å¯¹æ¯” (æ¯«ç±³):

æŒ‡æ ‡                            å¢å¼º+FPS            KæŠ˜æ¨¡å‹         æ”¹è¿›

---

MSE(mm^2)                 161.163361      158.556595     -1.64%

RMSE(mm)                   12.695013       12.591926     -0.82%

MAE(mm)                    10.748398        9.982676     -7.67%



æ¯ä¸ªåœ°æ ‡ç‚¹3Dè¯¯å·®å¯¹æ¯” (RMSE, mm):

åœ°æ ‡ç‚¹                   å¢å¼º+FPS         KæŠ˜æ¨¡å‹         æ”¹è¿›

---

Glabella           26.896006    25.893202     -3.87%

Nasion             19.449949    25.875690     24.83%

Rhinion            20.171515    22.846302     11.71%

Nasal Tip          14.824083    18.640568     20.47%

Subnasale          25.170589    17.676407    -42.40%

Alare (R)          18.332071    21.040945     12.87%

Alare (L)          23.449505    13.022996    -80.06%

Zygion (R)         17.482834    30.115946     41.95%

Zygion (L)         28.247475    15.487059    -82.39%



ğŸ“ è¯¦ç»†ç»“æœå·²ä¿å­˜: C:\\Users\\mkale\\Desktop\\Pointnet\_Pointnet2\_pytorch-master\\PointFeatureProject\\results\\test\_comparison\_aug\_fps\_vs\_kfold\_aug\_fps.json



#### 2/3 note hu

1. **Ran hyperparameter tuning (random search, 20 trials) on K-fold + FPS + å¢å¼º.**
   Best params: LR 0.0015, BS 8, Dropout 0.35, Step 120, Gamma 0.7, FT 0.001.

* Validation mean loss improved 0.000254 â†’ 0.00007065 (â‰ˆ72%).
* Added scripts: main\_script\_tuned\_kfold.py (train with best params, 300 epochs, 5 folds), evaluate\_tuned\_model\_comprehensive.py (test compare vs all historical models), analyze\_hyperparameter\_tuning.py (summaries).
* Pending: run tuned training + comprehensive test evaluation to get final mm metrics.



**2. Train with tuned hyperparameter**



âœ¨ æœ€ä½³æŠ˜: Fold 4

Â   æœ€ä½³éªŒè¯æŸå¤±: 0.00001178



ğŸ“Š K æŠ˜ç»Ÿè®¡:

Â   å¹³å‡éªŒè¯æŸå¤±: 0.00001726 Â± 0.00000279



ğŸ’¾ æœ€ä½³æ¨¡å‹å·²ä¿å­˜: C:\\Users\\mkale\\Desktop\\Pointnet\_Pointnet2\_pytorch-master\\PointFeatureProject\\models\\pointnet\_regression\_model\_tuned\_kfold\_best.pth

ğŸ“ è®­ç»ƒå†å²å·²ä¿å­˜: C:\\Users\\mkale\\Desktop\\Pointnet\_Pointnet2\_pytorch-master\\PointFeatureProject\\results\\training\_histories\\training\_history\_tuned\_kfold.json

================================================================================



================================================================================

ğŸ§ª è¶…å‚æ•°è°ƒä¼˜æ¨¡å‹ vs æ‰€æœ‰å†å²æ¨¡å‹ - æµ‹è¯•é›†å¯¹æ¯”

================================================================================



æµ‹è¯•é›†æ ·æœ¬æ•°: 20



================================================================================

è¯„ä¼°æ¨¡å‹: è¶…å‚æ•°è°ƒä¼˜KæŠ˜æ¨¡å‹

================================================================================

Â  RMSE: 13.3969 mm

Â  MAE:  11.2938 mm



================================================================================

è¯„ä¼°æ¨¡å‹: KæŠ˜+å¢å¼º+FPS

================================================================================

Â  RMSE: 12.5920 mm

Â  MAE:  9.9827 mm



================================================================================

è¯„ä¼°æ¨¡å‹: å•æ¬¡è®­ç»ƒ+å¢å¼º+FPS

================================================================================

Â  RMSE: 12.6953 mm

Â  MAE:  10.7486 mm



================================================================================

è¯„ä¼°æ¨¡å‹: æ—§KæŠ˜æ¨¡å‹ï¼ˆæ— å¢å¼ºï¼‰

================================================================================

Â  RMSE: 13.8366 mm

Â  MAE:  10.9962 mm



================================================================================

ğŸ“Š æµ‹è¯•é›†æ€§èƒ½å¯¹æ¯” (æ¯«ç±³)

================================================================================



æ¨¡å‹                                    RMSE(mm)         MAE(mm)          vs æ—§KæŠ˜

---

è¶…å‚æ•°è°ƒä¼˜KæŠ˜æ¨¡å‹                              13.3969         11.2938          +3.18%

KæŠ˜+å¢å¼º+FPS                              12.5920          9.9827          +8.99%

å•æ¬¡è®­ç»ƒ+å¢å¼º+FPS                            12.6953         10.7486          +8.25%

æ—§KæŠ˜æ¨¡å‹ï¼ˆæ— å¢å¼ºï¼‰                             13.8366         10.9962          +0.00%



================================================================================

ğŸ“ æ¯ä¸ªåœ°æ ‡ç‚¹ RMSE å¯¹æ¯” (mm)

================================================================================



åœ°æ ‡ç‚¹                  è¶…å‚æ•°è°ƒä¼˜KæŠ˜æ¨¡å‹      KæŠ˜+å¢å¼º+FPS    å•æ¬¡è®­ç»ƒ+å¢å¼º+FPS     æ—§KæŠ˜æ¨¡å‹ï¼ˆæ— å¢å¼ºï¼‰

---

Glabella               20.0195        25.8929        26.8965        17.0340

Nasion                 24.5034        25.8761        19.4501        34.7037

Rhinion                27.7863        22.8462        20.1718        26.5136

Nasal Tip              25.8702        18.6410        14.8243        25.1121

Subnasale              20.7761        17.6761        25.1705        12.6844

Alare (R)              17.4042        21.0409        18.3322        24.8110

Alare (L)              22.1051        13.0236        23.4497        24.9964

Zygion (R)             22.2331        30.1159        17.4840        18.0875

Zygion (L)             26.1885        15.4877        28.2487        24.7512



ğŸ“ è¯¦ç»†ç»“æœå·²ä¿å­˜: C:\\Users\\mkale\\Desktop\\Pointnet\_Pointnet2\_pytorch-master\\PointFeatureProject\\results\\test\_evaluations\\test\_comparison\_all\_models\_with\_tuned.json



================================================================================

ğŸ’¡ ç»“è®º

================================================================================



ğŸ¯ è¶…å‚æ•°è°ƒä¼˜æ¨¡å‹æµ‹è¯•é›† RMSE: 13.3969 mm

Â   ç›¸æ¯”æ—§KæŠ˜æ¨¡å‹æ”¹è¿›: 3.18%



âš ï¸  è·ç¦» 2mm ç›®æ ‡ä»æœ‰å·®è·ï¼Œå»ºè®®:

Â   1. æ”¶é›†æ›´å¤šè®­ç»ƒæ ·æœ¬

Â   2. å°è¯• PointNet++ æ¶æ„

Â   3. åˆ†æé«˜è¯¯å·®åœ°æ ‡ç‚¹ç‰¹å¾

================================================================================



**3. Train with pointnet++ : using 5-fold cross-validation with data augmentation and farthest-point sampling (8192 points), 90/10 of train/test split**



===== åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼° =====

C:\\Users\\mkale\\Desktop\\Pointnet\_Pointnet2\_pytorch-master\\PointFeatureProject\\scripts\\training\\main\_script\_pointnet2\_kfold.py:239: FutureWarning: You are using `torch.load` with `weights\_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights\_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add\_safe\_globals`. We recommend you start setting `weights\_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.

&nbsp; best\_model.load\_state\_dict(torch.load(best\_overall\_model))

æµ‹è¯•é›†æŸå¤±: 0.000025

L2è·ç¦» (å¹³å‡): 0.008098 Â± 0.003017

å„åœ°æ ‡L2è·ç¦»: \[0.00828781 0.00799164 0.0060118  0.00537658 0.00875341 0.00886916

&nbsp;0.00870771 0.00947082 0.00941711]



==== è®­ç»ƒå®Œæˆ ====

æœ€ä½³æŠ˜: Fold 5, æœ€ä½³éªŒè¯æŸå¤±: 0.000009

å¹³å‡éªŒè¯æŸå¤±: 0.000012 Â± 0.000002

æµ‹è¯•é›†æŸå¤±: 0.000025

æœ€ä½³æ¨¡å‹: C:\\Users\\mkale\\Desktop\\Pointnet\_Pointnet2\_pytorch-master\\PointFeatureProject\\models\\pointnet2\_regression\_kfold\_best.pth

è®­ç»ƒå†å²: C:\\Users\\mkale\\Desktop\\Pointnet\_Pointnet2\_pytorch-master\\PointFeatureProject\\results\\training\_histories\\training\_history\_pointnet2\_kfold.json

================================================================================
EVALUATION RESULTS
================================================================================

Number of samples: 10
Number of landmarks: 9

--------------------------------------------------------------------------------
L2 DISTANCE - NORMALIZED SPACE
--------------------------------------------------------------------------------
  Mean: 0.008148
  Std:  0.002876
  Per-landmark:
    Landmark 1     : 0.008222
    Landmark 2     : 0.008244
    Landmark 3     : 0.006237
    Landmark 4     : 0.005251
    Landmark 5     : 0.008664
    Landmark 6     : 0.009041
    Landmark 7     : 0.008762
    Landmark 8     : 0.009527
    Landmark 9     : 0.009382



**4. PointNet++ Hyperparameter Search with Grid Search**

Created `scripts/training/hparam_search_pointnet2.py` to systematically search best hyperparameters for PointNet++ regression model.

ğŸ“‹ **Search Space:**
   - Learning Rate: [1e-3, 5e-4]
   - Dropout: [0.3, 0.4]
   - Weight Decay: [0.0, 1e-4]
   - Loss Type: ["mse", "smoothl1"]
   - Geometry Lambda: [0.0, 0.1] (pairwise distance regularization)
   - SA1 Radii: [[0.1,0.2,0.4], [0.05,0.1,0.2]]
   - SA2 Radii: [[0.2,0.4,0.8], [0.1,0.2,0.4]]
   - Total: 2Ã—2Ã—2Ã—2Ã—2Ã—2Ã—2 = 128 configurations

ğŸ”§ **Key Improvements:**
   1. **FPS Device Fix:** `farthest_point_sampling()` now checks device type - uses GPU-accelerated `furthest_point_sample` on CUDA, falls back to random sampling on CPU (prevents crash)
   2. **GPU Augmentation:** Moved data augmentation to GPU by transferring batches to device BEFORE `augment_batch()` call (rotation/scale/shift/jitter all run on GPU tensors)
   3. **Val L2 Selection:** Tracks validation L2 mean (in normalized space) per epoch; selects best config by lowest val L2 mean instead of loss (more aligned with evaluation metric)
   4. **No Val Leakage in Final Run:** Retrains best config on train+val for FINAL_EPOCHS (160) WITHOUT validation-based early stopping - saves last epoch weights to avoid leaking test info

ğŸ“Š **Training Pipeline:**
   - Search phase: 80 epochs per config on train/val split (90% train â†’ 80% train + 20% val from it)
   - Selection: picks config with lowest validation L2 mean across all landmarks
   - Final phase: retrains best config on full train+val (90%) for 160 epochs, evaluates on held-out test (10%)

ğŸ’¾ **Outputs:**
   - Best model: `models/pointnet2_regression_hparam_best.pth`
   - Summary JSON: `results/training_histories/hparam_search_pointnet2.json` (includes all search results, best config, test metrics)
   - Log file: `results/logs/hparam_search_pointnet2_<timestamp>.log`

ğŸ¯ **Next:** Run grid search to find optimal hyperparameters for PointNet++ architecture.


================================================================================

**2024-02-04: PointNet++ Hyperparameter Grid Search Results**

================================================================================

âœ… **Grid Search Completed Successfully**

ğŸ“… **Training Period:** 2026-02-03 16:48 â†’ 2026-02-04 02:09 (çº¦9.5å°æ—¶)

ğŸ“‹ **Search Scope:**
   - Total configurations tested: 128
   - Each config trained for 80 epochs on train/val split
   - Selection metric: Validation L2 mean distance (normalized space)

ğŸ† **Best Configuration Found:**
```python
{
    'lr': 0.001,
    'dropout': 0.4,
    'weight_decay': 0.0,
    'loss_type': 'smoothl1',
    'geo_lambda': 0.0,
    'sa1_radii': [0.1, 0.2, 0.4],
    'sa2_radii': [0.2, 0.4, 0.8]
}
```
   - Best validation L2 mean: **0.010362**

ğŸ“Š **Final Model Training (Best Config on Train+Val):**
   - Duration: 160 epochs on full training+validation set (90% of data)
   - No validation split in final training (avoids leakage)
   - Training loss progression:
     * Epoch 20/160:  0.013536
     * Epoch 40/160:  0.003063
     * Epoch 60/160:  0.001662
     * Epoch 80/160:  0.000567
     * Epoch 100/160: 0.000255
     * Epoch 120/160: 0.000149
     * Epoch 140/160: 0.000103
     * Epoch 160/160: 0.000091

================================================================================
TEST SET EVALUATION RESULTS (10% held-out data)
================================================================================

ğŸ“ **Overall Metrics:**
   - Test Loss: 9.357e-06
   - L2 Mean Distance: **0.007009 mm** (normalized space)
   - L2 Std: 0.002648 mm

ğŸ“ **Per-Landmark L2 Distance (mm):**
   1. Landmark 1: 0.006378
   2. Landmark 2: 0.008842
   3. Landmark 3: 0.006260
   4. Landmark 4: 0.006986
   5. Landmark 5: 0.007560
   6. Landmark 6: 0.005382 â­ (best)
   7. Landmark 7: 0.005561
   8. Landmark 8: 0.008487
   9. Landmark 9: 0.007626

ğŸ’¡ **Key Findings:**
   1. **Dropout matters:** Best config uses higher dropout (0.4) to prevent overfitting
   2. **Smooth L1 loss superior:** SmoothL1 loss outperforms MSE for landmark regression
   3. **No geometry regularization needed:** geo_lambda=0.0 works best (pairwise distance constraints not helpful)
   4. **Weight decay not required:** Best config has weight_decay=0.0
   5. **Multi-scale radii optimal:** SA radii [0.1,0.2,0.4] and [0.2,0.4,0.8] capture local+global features well

ğŸ“ **Model Location:**
   - Final trained model: `models/pointnet2_regression_hparam_best.pth`
   - Training history: `results/training_histories/hparam_search_pointnet2.json`
   - Detailed log: `results/logs/hparam_search_pointnet2_20260203_164816.log`

ğŸ¯ **Performance Improvement:**
   - Previous best (PointNet++ K-fold): L2 = 0.008148 mm
   - **This model (Hyperparameter tuned): L2 = 0.007009 mm**
   - **Improvement: 14.0% reduction in error** ğŸ‰

ğŸ”¬ **Next Steps:**
   - Analyze which landmarks benefit most from hyperparameter tuning
   - Consider ensemble methods combining best configurations
   - Investigate data augmentation strategies for high-error landmarks (e.g., Landmark 2, 8)

================================================================================

